{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Portfolio Assignment week 01\n",
    "\n",
    "Study the Tutorial tutorial_cluster_scanpy_object and the tutorial_Clustering_Methods\n",
    "\n",
    "Write a brief summary about the following:\n",
    "\n",
    "-\tWhat are common preprocessing steps? Explain for each step why and when you should execute this step and when not.\n",
    "\n",
    "If someone wants to elucidate the preprocessing step from the very beginning, they can commence by framing the problem. During this stage, individuals are empowered to observe the problem, comprehend its nature, construct a framework to explain it, and contemplate various potential solutions and analytical tools that can be employed to address the issue or prove a hypothesis.\n",
    "\n",
    "In the subsequent step, it is essential to gather data in a proper manner. An illustrative example is our teacher's experience as a consular in a potato company, where she encountered a dataset that was handwritten in a notebook instead of being stored securely on a hard drive. As a result, her initial task involved consolidating this fragmented dataset into a file while ensuring its integrity, before proceeding with the subsequent steps.\n",
    "\n",
    "The next step involves loading and inspecting the dataset. In my perspective, the inspection of the dataset is a crucial aspect of data analysis and machine learning. During this phase, researchers can gain familiarity with all aspects of the dataset, identify potential anomalies, and ascertain the rationale behind selecting the most appropriate methods for further investigation. Some of the information that one should evaluate during this stage includes:\n",
    "\n",
    "1- Sometimes the dataset may lack the proper format or structure, making it unsuitable for storage as a Pandas or NumPy array. In such scenarios, the initial step involves data cleaning, which entails slicing the relavant part of the data, changing its format if is needed. Alternatively, the dataset might include low-quality data points that require removal or careful handling prior to conducting further investigations.\n",
    "\n",
    "2- Gain a deeper understanding of the features within the dataset. It is important to determine the number of features present and classify them as continuous, categorical, or boolean. Additionally, explore how the dataset is distributed based on categorical features to assess its balance. Extract relevant statistical information about the features, including mean values and standard deviation. Evaluate the range of values for continuous features and their variance to determine if standardization is necessary.\n",
    "\n",
    "3- Create a correlation matrix to identify potential strong relationships between features. If a strong correlation is found between two features, considering only one of them in the model can suffice, as it can capture the variation of both. This approach helps avoiding overfitting. Additionally, evaluate the skewness of feature distributions, as certain machine learning estimators perform poorly when individual features deviate significantly from a standard normal distribution. To address skewness, various methods such as logarithmic transformation can be employed to minimize its impact.\n",
    "\n",
    "4- When dealing with multiple datasets, it is crucial for the researcher to identify the attributes (features) within each dataset and seek to establish relationships between them.\n",
    "\n",
    "5- Identify the presence of null values within each dataset and employ appropriate strategies to handle them. This can involve removing rows or columns that contain null values, replacing them with suitable values such as zeros, or utilizing interpolation techniques.\n",
    "\n",
    "6- Thoroughly inspect the dataset for anomalies or incorrect information, such as erroneous IDs or data points that significantly deviate from the expected values within a feature. Additionally, identify any features that may contain different types of variables due to mistakes or intentional modifications. While the detection of such anomalies can be addressed using machine learning techniques in the anomaly detection phase, investigating the dataset's features can provide early indications of potential data issues.\n",
    "\n",
    "The final step involves preparing and splitting the dataset. During this stage, it is necessary to address any issues identified in the previous steps and then split the part of the data set that will be needed in further investigation. If multiple datasets are available, one can concatenate them to create a unified dataset and subsequently extract the required portion for analysis.\n",
    " \n",
    "-\tWhat visualization methods are used in the cluster methods tutorial? Explain why the selected method is the most appropriate method for the visualization. Bonus points: do this as well for the scanpy tutorial.\n",
    "\n",
    "Indeed, different visualization methods were used in this tutorial. All of them will explain in following paragraphs:\n",
    "\n",
    "1- Histogram: This visualization method is utilized to depict the quality distribution for each wine color. By employing this method, one can effectively analyze the distribution of quality based on color. Moreover, this method enables not only a comparison of sample abundance across different colors but also a comparison of quality distributions between colors.\n",
    "\n",
    "2- pairplots: This tool is considered one of the most powerful tools for data inspection and analysis as it provides scientists with an immediate overview of category separation within the data. In this context, the tool is utilized to examine the pairwise distribution of variables, serving to validate scaling and normalization efforts. It also facilitates a comparison of pairwise distributions, allowing for the identification of differences between categories within variables. Additionally, the diagonal elements of this plot consist of histograms representing different variables, enabling further comparisons across categories. Overall, the utilization of a pairplot enhances understanding of the dataset and its variables.\n",
    "\n",
    "3- Inertia plot: This plot serves as a valuable tool for determining the optimal number of clusters when utilizing the K-means method. The only additional parameter required for the K-means function is the estimated number of clusters. One approach to estimating this number is by analyzing the Inertia plot. By observing the plot, one can identify an \"elbow\" point, which indicates the estimated number of clusters. \n",
    "\n",
    "4- Dendrogram: This is often accompanied by Hierarchical Agglomerative Clustering (HAC), as it enables users to track the merging process of each pair of data points into larger groups at each iteration. Dendrogram provides information about both the similarity between data points and the order in which clusters are formed. The first-formed cluster in it always consists of the most similar variables and has the shortest branch. Conversely, the last-formed cluster encompasses all the variables and has the longest branch. Consequently, one can deduce the relationships between variables within a cluster as well as between clusters within a dataset.\n",
    "\n",
    "5- average ROC-AUC scores plot (Line Plot): In the context of clustering, the optimal number of clusters can be determined by examining the average scores on the ROC-AUC curve. The highest average score corresponds to the number that can best predict the ideal number of clusters.\n",
    "\n",
    "-\tWhat performance/evaluation metrics are in the cluster methods tutorial? Explain why the used methods are the most appropriate method for the evaluation.\n",
    " \n",
    "1- Inertia: The Inertia illustrates the reduction in the sum of squared distances of samples to their closest cluster center. Initially, the inertia starts from a large value and gradually decreases towards zero if the number of clusters equals the number of data points. Beyond a certain number of clusters, the reduction in inertia becomes relatively constant, suggesting that additional clusters are subclusters rather than distinct main clusters. This method is the best because one can determine the elbow or the actual number of clusters for a dataset, and in this scenario this should be done.\n",
    "\n",
    "2- average ROC-AUC scores: AUC, which stands for \"Area under the ROC Curve,\" quantifies the complete two-dimensional area beneath the entire ROC curve, spanning from (0,0) to (1,1). It provides a comprehensive measure of performance across all possible classification thresholds. The ROC curve, on the other hand, is a graphical representation that depicts the diagnostic capability of a binary classifier system as the discrimination threshold is adjusted [Wiki]. The ROC curve is constructed by plotting the true positive rate (TPR) against the false positive rate (FPR) across different threshold settings. The true-positive rate is also known as sensitivity, or the probability of detection. Conversely, the false positive rate is referred to as the probability of false alarm and can be calculated as (1 - specificity). This method is mostly used to evaluate the credibility of a clustering technique. In this case, the credibility of K-means clustering method for wine dataset should be evaluated. So, one of the best methods to do this is average ROC-AUC scores.\n",
    "\n",
    "Bonus:\n",
    "You practice the steps yourself with the breast_cancer dataset (clustering_data.csv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
