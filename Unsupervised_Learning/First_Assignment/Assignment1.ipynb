{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Portfolio Assignment week 01\n",
    "\n",
    "Study the Tutorial tutorial_cluster_scanpy_object and the tutorial_Clustering_Methods\n",
    "\n",
    "Write a brief summary about the following:\n",
    "\n",
    "-\tWhat are common preprocessing steps? Explain for each step why and when you should execute this step and when not.\n",
    "\n",
    "If someone wants to elucidate the preprocessing step from the very beginning, they can commence by framing the problem. During this stage, individuals are empowered to observe the problem, comprehend its nature, construct a framework to explain it, and contemplate various potential solutions and analytical tools that can be employed to address the issue or prove a hypothesis.\n",
    "\n",
    "In the subsequent step, it is essential to gather data in a proper manner. An illustrative example is our teacher's experience as a consular in a potato company, where she encountered a dataset that was handwritten in a notebook instead of being stored securely on a hard drive. As a result, her initial task involved consolidating this fragmented dataset into a file while ensuring its integrity, before proceeding with the subsequent steps.\n",
    "\n",
    "The next step involves loading and inspecting the dataset. In my perspective, the inspection of the dataset is a crucial aspect of data analysis and machine learning. During this phase, researchers can gain familiarity with all aspects of the dataset, identify potential anomalies, and ascertain the rationale behind selecting the most appropriate methods for further investigation. Some of the information that one should evaluate during this stage includes:\n",
    "\n",
    "1- Sometimes the dataset may lack the proper format or structure, making it unsuitable for storage as a Pandas or NumPy array. In such scenarios, the initial step involves data cleaning, which entails slicing the relavant part of the data, changing its format if is needed. Alternatively, the dataset might include low-quality data points that require removal or careful handling prior to conducting further investigations.\n",
    "\n",
    "2- Gain a deeper understanding of the features within the dataset. It is important to determine the number of features present and classify them as continuous, categorical, or boolean. Additionally, explore how the dataset is distributed based on categorical features to assess its balance. Extract relevant statistical information about the features, including mean values and standard deviation. Evaluate the range of values for continuous features and their variance to determine if standardization is necessary.\n",
    "\n",
    "3- Create a correlation matrix to identify potential strong relationships between features. If a strong correlation is found between two features, considering only one of them in the model can suffice, as it can capture the variation of both. This approach helps avoiding overfitting. Additionally, evaluate the skewness of feature distributions, as certain machine learning estimators perform poorly when individual features deviate significantly from a standard normal distribution. To address skewness, various methods such as logarithmic transformation can be employed to minimize its impact.\n",
    "\n",
    "4- When dealing with multiple datasets, it is crucial for the researcher to identify the attributes (features) within each dataset and seek to establish relationships between them.\n",
    "\n",
    "5- Identify the presence of null values within each dataset and employ appropriate strategies to handle them. This can involve removing rows or columns that contain null values, replacing them with suitable values such as zeros, or utilizing interpolation techniques.\n",
    "\n",
    "6- Thoroughly inspect the dataset for anomalies or incorrect information, such as erroneous IDs or data points that significantly deviate from the expected values within a feature. Additionally, identify any features that may contain different types of variables due to mistakes or intentional modifications. While the detection of such anomalies can be addressed using machine learning techniques in the anomaly detection phase, investigating the dataset's features can provide early indications of potential data issues.\n",
    "\n",
    "The final step involves preparing and splitting the dataset. During this stage, it is necessary to address any issues identified in the previous steps and then split the part of the data set that will be needed in further investigation. If multiple datasets are available, one can concatenate them to create a unified dataset and subsequently extract the required portion for analysis.\n",
    " \n",
    "-\tWhat visualization methods are used in the cluster methods tutorial? Explain why the selected method is the most appropriate method for the visualization. Bonus points: do this as well for the scanpy tutorial.\n",
    "-\tWhat performance/evaluation metrics are in the cluster methods tutorial? Explain why the used methods are the most appropriate method for the evaluation.\n",
    "\n",
    "\n",
    "Bonus:\n",
    "You practice the steps yourself with the breast_cancer dataset (clustering_data.csv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
