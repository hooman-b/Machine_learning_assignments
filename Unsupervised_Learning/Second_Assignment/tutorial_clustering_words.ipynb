{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1bef54c3",
   "metadata": {},
   "source": [
    "# Material added\n",
    "this notebook contains a study on NMF method. So, I will add some visualization and analytical methods. \n",
    "# Clustering text\n",
    "\n",
    "See also https://towardsdatascience.com/nmf-a-visual-explainer-and-python-implementation-7ecdd73491f8\n",
    "\n",
    "\n",
    "This notebook uses a collection of clinical case reports to cluster words by topics using the NMF method. To cluster text we need to preprocess the text first with regular natural language processing cleaning steps such as remove punctuations, stopwords, or other unwanted text. we lower the text and use the lemma to reduce variation of words. This is all done in part A. \n",
    "\n",
    "Next we need to prepare the text in a document term matrix so that NMF can perform the calculations. The Document-Term Matrix (DTM) represents the frequency of words (or terms) in a collection of documents. Each row in the matrix represents a document, and each column represents a word in the vocabulary. The value in each cell represents the frequency of the corresponding word in the corresponding document. This is done in part B\n",
    "\n",
    "Lastly we run the clustering algorithm and visualize the outcome. This is done in part C\n",
    "\n",
    "\n",
    "## The data \n",
    "A collection of 200 clinical case report documents in plain text format are used. The documents are named using PubMed document IDs, and have been edited to include only clinical case report details. The dataset is called \"MACCROBAT2020\" and is the second release of this dataset, with improvements made to the consistency and format of annotations\n",
    "https://figshare.com/articles/dataset/MACCROBAT2018/9764942\n",
    "\n",
    "\n",
    "## Portfolio assignment\n",
    "\n",
    "You can use this assignment to fill your portfolio.\n",
    "Read, execute and analyse the code in the notebook. Then *choose one* of the assignments a), b) or c). \n",
    "\n",
    "a) read the article Clinical Documents Clustering Based on Medication/Symptom Names Using Multi-View Nonnegative Matrix Factorization. you can find the article <a href = 'https://pubmed.ncbi.nlm.nih.gov/26011887/'> here</a>. Explain the similarities of this notebook and the article. Explain in your own words what need to be added to this notebook to reproduce the article. There is no need to code the solution.\n",
    "\n",
    "b) Improve the outcome improving the data preprocessing and the hyper parameter configurations. Explain your choices. Your solution should be a coded solution with comments. Are there any other weighting solutions next to TF-IDF?\n",
    "\n",
    "c) Provide a text clustering solution with your own data of interest, for instance text you work with in your project. \n",
    "\n",
    "Mind you that you are not allowed to copy code solutions without referencing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4f823c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.feature_extraction import text\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import os\n",
    "import string\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Slicing dictionaries\n",
    "import itertools\n",
    "from pathlib import Path\n",
    "\n",
    "# Analyzing and Visualizing modules\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc0580aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Parsian\n",
      "[nltk_data]     computer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b58de96a",
   "metadata": {},
   "source": [
    "# Part A: get and clean the text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "87fa9acc",
   "metadata": {},
   "source": [
    "## Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c8ec4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('D:\\\\Data Science for Life Science\\\\Machine Learning and Deep learning\\\\Data\\\\Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "553dddc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create empty dataframe\n",
    "df = pd.DataFrame(columns=['docid','text'])\n",
    "\n",
    "# get all files' names ending with .txt\n",
    "docs = [x for x in glob.glob(\"*.txt\")]\n",
    "\n",
    "#fill dataframe\n",
    "for doc in docs:\n",
    "    txt = Path(doc).read_text()\n",
    "    df.loc[len(df.index)] = [doc[:-4], txt]\n",
    "\n",
    "#set index docid\n",
    "df = df.set_index('docid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbfb7ee9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>docid</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15939911</th>\n",
       "      <td>CASE: A 28-year-old previously healthy man pre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16778410</th>\n",
       "      <td>The patient was a 34-yr-old man who presented ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17803823</th>\n",
       "      <td>A 23 year old white male with a 4 year history...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18236639</th>\n",
       "      <td>A 30-year-old female (65 kg) underwent rhinopl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18258107</th>\n",
       "      <td>Here, we describe another case in a 60-year-ol...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       text\n",
       "docid                                                      \n",
       "15939911  CASE: A 28-year-old previously healthy man pre...\n",
       "16778410  The patient was a 34-yr-old man who presented ...\n",
       "17803823  A 23 year old white male with a 4 year history...\n",
       "18236639  A 30-year-old female (65 kg) underwent rhinopl...\n",
       "18258107  Here, we describe another case in a 60-year-ol..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a8564d7a",
   "metadata": {},
   "source": [
    "## Cleaning the text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "98d33dd2",
   "metadata": {},
   "source": [
    "The code below defines a function called clean_text that takes a text parameter and returns a cleaned version of it. The function first converts the text to lowercase using the `lower()` method. It then removes any text enclosed in square brackets using **regular expressions** and the re.sub() method. Next, it removes any **punctuation** from the text using the `string.punctuation` module and `re.escape()` and `re.sub()` methods. It then removes any words that **contain numbers** using `re.sub()` and a regular expression. Finally, it removes any **read errors** (represented by the ï¿½ character) using `re.sub()`.\n",
    "\n",
    "The code then defines a lambda function called cleaned that takes a single parameter `x` and applies the `clean_text` function to it. This lambda function can be used to clean text data in a Pandas dataframe or any other data structure that can take a lambda function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03f04d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    '''Make text lowercase, remove text in square brackets, \n",
    "    remove punctuation, remove read errors,\n",
    "    and remove words containing numbers.'''\n",
    "    text = text.lower()\n",
    "    # replace anything that exists in a square bracket with space\n",
    "    text = re.sub('\\[.*?\\]', ' ', text)\n",
    "    # %s means its a string, %d is an integer, %f is floating point number.\n",
    "    # escape is a function that escapes any special characters present in the the string.\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), ' ', text)\n",
    "    # '\\w*\\d\\w*' is a regular expression pattern that matches any word that contains at least one digit.\n",
    "    text = re.sub('\\w*\\d\\w*', ' ', text)\n",
    "    # replace the errors with space\n",
    "    text = re.sub('ï¿½', ' ', text)\n",
    "    return text\n",
    "\n",
    "cleaned = lambda x: clean_text(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fba7a135",
   "metadata": {},
   "source": [
    "The function 'nouns' cleans a text and extracts only the nouns from it. It first defines a lambda function called 'is_noun' that checks if a given word is a noun, based on its part of speech (POS) tag. The input text is then tokenized using the `word_tokenize` function from the **nltk** module, which breaks down the text into smaller units called tokens. Tokenization is an important pre-processing step in NLP that helps standardize and prepare text data for further analysis.\n",
    "\n",
    "The function creates a `WordNetLemmatizer` object from the nltk module to lemmatize each word, which is the process of converting a word to its base or dictionary form. Lemmatization helps to reduce the dimensionality of text data and improve the accuracy of the analysis, especially in cases where words have different inflected forms but share the same root.\n",
    "\n",
    "The function uses a list comprehension to lemmatize each word in the tokenized text if it is a noun, and stores the resulting list of lemmatized nouns in the 'all_nouns' variable. Finally, the function returns a string containing the joined list of lemmatized nouns.\n",
    "\n",
    "Note that this function requires the 'nltk' module to be imported and assumes that the 'pos_tag' function from the 'nltk' module is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82d3eaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noun extract and lemmatize function\n",
    "def nouns(text):\n",
    "    '''Given a string of text, tokenize the text \n",
    "    and pull out only the nouns.'''\n",
    "    # create mask to isolate words that are nouns\n",
    "    is_noun = lambda pos: pos[:2] == 'NN'\n",
    "    # store function to split string of words \n",
    "    # into a list of words (tokens)\n",
    "    tokenized = word_tokenize(text)\n",
    "    # store function to lemmatize each word\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    # use list comprehension to lemmatize all words \n",
    "    # and create a list of all nouns\n",
    "    all_nouns = [wordnet_lemmatizer.lemmatize(word) \\\n",
    "    for (word, pos) in pos_tag(tokenized) if is_noun(pos)] \n",
    "    \n",
    "    #return string of joined list of nouns\n",
    "    return ' '.join(all_nouns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20d47d53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>docid</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15939911</th>\n",
       "      <td>case year man week history palpitation symptom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16778410</th>\n",
       "      <td>patient man complaint fever cough smoker histo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17803823</th>\n",
       "      <td>year male year history crohn disease day histo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18236639</th>\n",
       "      <td>year female kg rhinoplasty anaesthesia combina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18258107</th>\n",
       "      <td>case year man francisco pork philippine june m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       text\n",
       "docid                                                      \n",
       "15939911  case year man week history palpitation symptom...\n",
       "16778410  patient man complaint fever cough smoker histo...\n",
       "17803823  year male year history crohn disease day histo...\n",
       "18236639  year female kg rhinoplasty anaesthesia combina...\n",
       "18258107  case year man francisco pork philippine june m..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Clean Text\n",
    "df[\"text\"] = df[\"text\"].apply(cleaned)\n",
    "data_nouns = pd.DataFrame(df[\"text\"].apply(nouns))\n",
    "# Visually Inspect\n",
    "data_nouns.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bd934e72",
   "metadata": {},
   "source": [
    "# Part B: The Document-Term Matrix (DTM)\n",
    "\n",
    "To perform analyses we need to create a Document-Term Maxtrix. \n",
    "The Document-Term Matrix (DTM) represents the frequency of words (or terms) in a collection of documents. Each row in the matrix represents a document, and each column represents a word in the vocabulary. The value in each cell represents the frequency of the corresponding word in the corresponding document.\n",
    "\n",
    "In specific case below, the DTM has been created using only the nouns extracted from the original text, and has been transformed using the TF-IDF (Term Frequency-Inverse Document Frequency) weighting scheme. The TF-IDF scheme assigns weights to words based on how often they appear in a document relative to how often they appear in the entire corpus. Words that appear frequently in a document but infrequently in the corpus are given higher weights, as they are considered to be more important for distinguishing that document from others in the corpus. This weighting scheme is commonly used in text mining and information retrieval to identify key terms or topics in a collection of documents.\n",
    "\n",
    "The resulting DTM can be used for various purposes such as topic modeling, clustering, classification, and visualization of text data.\n",
    "\n",
    "#### TfidfVectorizer: \n",
    "This vectorizer will tokenize the text, filter out English stop words, compute the TF-IDF (Term Frequency-Inverse Document Frequency) representation, and only include terms that meet the specified document frequency thresholds. It will consider only unigrams and exclude terms that appear in more than 80% of the documents or less than 1% of the documents\n",
    "\n",
    "#### stop_words=text.ENGLISH_STOP_WORDS:\n",
    "This parameter specifies the stop words to be used for filtering during the vectorization process. The value text.ENGLISH_STOP_WORDS suggests that the stop words from the text module in scikit-learn are being used. These are commonly used English words that are often considered insignificant for analysis purposes and are typically excluded from text processing tasks.\n",
    "\n",
    "#### ngram_range:\n",
    "An n-gram is a contiguous sequence of n items from a given sample of text. In the context of text processing, an item can be a word or a character. The ngram_range parameter allows you to control the range of n-grams that will be generated from the text data.\n",
    "\n",
    "The ngram_range parameter takes a tuple as its value, with two elements: (min_n, max_n). Here's what each element represents\n",
    "\n",
    "    min_n: It specifies the minimum value of n for the range of n-grams. This means that the minimum number of consecutive items (words or characters) to be considered is min_n.\n",
    "    \n",
    "    max_n: It specifies the maximum value of n for the range of n-grams. This means that the maximum number of consecutive items (words or characters) to be considered is max_n.\n",
    "\n",
    "By adjusting the ngram_range parameter, you can control the level of granularity and contextual information captured in the n-grams during the text vectorization process.\n",
    "\n",
    "#### max_df=0.8: \n",
    "This parameter sets the maximum document frequency for a term to be included in the vectorization. A value of 0.8 means that any term that appears in more than 80% of the documents will be ignored.\n",
    "\n",
    "#### min_df=0.01:\n",
    " This parameter sets the minimum document frequency for a term to be included in the vectorization. A value of 0.01 means that a term must appear in at least 1% of the documents to be considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0fec90f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abdomen</th>\n",
       "      <th>ablation</th>\n",
       "      <th>abnormality</th>\n",
       "      <th>abscess</th>\n",
       "      <th>absence</th>\n",
       "      <th>absent</th>\n",
       "      <th>abuse</th>\n",
       "      <th>access</th>\n",
       "      <th>accompanying</th>\n",
       "      <th>accordance</th>\n",
       "      <th>...</th>\n",
       "      <th>york</th>\n",
       "      <th>yr</th>\n",
       "      <th>zhejiang</th>\n",
       "      <th>zinc</th>\n",
       "      <th>zone</th>\n",
       "      <th>Âµg</th>\n",
       "      <th>Âµmol</th>\n",
       "      <th>Î¼g</th>\n",
       "      <th>Î¼l</th>\n",
       "      <th>Î¼mol</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>docid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15939911</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.408857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16778410</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17803823</th>\n",
       "      <td>0.057677</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18236639</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.243491</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18258107</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.113573</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã 1959 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           abdomen  ablation  abnormality  abscess  absence  absent  abuse  \\\n",
       "docid                                                                        \n",
       "15939911  0.000000  0.408857          0.0      0.0      0.0     0.0    0.0   \n",
       "16778410  0.000000  0.000000          0.0      0.0      0.0     0.0    0.0   \n",
       "17803823  0.057677  0.000000          0.0      0.0      0.0     0.0    0.0   \n",
       "18236639  0.000000  0.000000          0.0      0.0      0.0     0.0    0.0   \n",
       "18258107  0.000000  0.000000          0.0      0.0      0.0     0.0    0.0   \n",
       "\n",
       "          access  accompanying  accordance  ...  york   yr  zhejiang  zinc  \\\n",
       "docid                                       ...                              \n",
       "15939911     0.0           0.0         0.0  ...   0.0  0.0       0.0   0.0   \n",
       "16778410     0.0           0.0         0.0  ...   0.0  0.0       0.0   0.0   \n",
       "17803823     0.0           0.0         0.0  ...   0.0  0.0       0.0   0.0   \n",
       "18236639     0.0           0.0         0.0  ...   0.0  0.0       0.0   0.0   \n",
       "18258107     0.0           0.0         0.0  ...   0.0  0.0       0.0   0.0   \n",
       "\n",
       "          zone   Âµg  Âµmol        Î¼g        Î¼l  Î¼mol  \n",
       "docid                                                \n",
       "15939911   0.0  0.0   0.0  0.000000  0.000000   0.0  \n",
       "16778410   0.0  0.0   0.0  0.000000  0.000000   0.0  \n",
       "17803823   0.0  0.0   0.0  0.000000  0.000000   0.0  \n",
       "18236639   0.0  0.0   0.0  0.243491  0.000000   0.0  \n",
       "18258107   0.0  0.0   0.0  0.000000  0.113573   0.0  \n",
       "\n",
       "[5 rows x 1959 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a document-term matrix with only nouns\n",
    "# Store TF-IDF Vectorizer\n",
    "tv_noun = TfidfVectorizer(stop_words=text.ENGLISH_STOP_WORDS, ngram_range = (1,1), max_df = .8, min_df = .01)\n",
    "# Fit and Transform speech noun text to a TF-IDF Doc-Term Matrix\n",
    "data_tv_noun = tv_noun.fit_transform(data_nouns.text)\n",
    "# Create data-frame of Doc-Term Matrix with nouns as column names\n",
    "# first it changes the data_tv_noun to an array and then use the nouns exist in tv_noun as the columns of the dataset\n",
    "data_dtm_noun = pd.DataFrame(data_tv_noun.toarray(), columns=tv_noun.get_feature_names_out())\n",
    "data_dtm_noun.index = df.index\n",
    "# Visually inspect Document Term Matrix\n",
    "data_dtm_noun.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9d0ad932",
   "metadata": {},
   "source": [
    "# Part C: Run the NMF\n",
    "\n",
    "This code below performs Non-negative Matrix Factorization (NMF) on the Document Term Matrix (DTM) with only nouns. NMF is a dimensionality reduction technique that decomposes a matrix into two matrices: a document-topic matrix (W) and a topic-term matrix (H).\n",
    "\n",
    "The code first creates an NMF model with 5 topics. The model is then fit to the DTM with the fit_transform() method, which returns the document-topic matrix (W).\n",
    "\n",
    "The `display_topics()` function is called to extract the top words from the topic-term matrix (H) using the feature names from the TF-IDF vectorizer. \n",
    "\n",
    "The resulting output will show the top n words for each of the n topics learned by the NMF model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd41d4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, num_top_words, topic_names=None):\n",
    "    '''Given an NMF model, feature_names, and number of top words, print \n",
    "       topic number and its top feature names, up to specified number of top words.'''\n",
    "    # iterate through topics in topic-term matrix, 'H' aka\n",
    "    # model.components_\n",
    "    for ix, topic in enumerate(model.components_):\n",
    "        #print topic, topic number, and top words\n",
    "        # if there is no topic or term print\n",
    "        if not topic_names or not topic_names[ix]:\n",
    "            print(\"\\nTopic \", ix)\n",
    "        # print topics and their most frequent words\n",
    "        else:\n",
    "            print(\"\\nTopic: '\",topic_names[ix],\"'\")\n",
    "        print(\", \".join([feature_names[i] \\\n",
    "             for i in topic.argsort()[:-num_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed21447b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  0\n",
      "dl, mg, level, blood, ml, day, count, platelet, serum, range\n",
      "\n",
      "Topic  1\n",
      "tumor, mass, cell, cm, lesion, figure, fig, lymph, resection, metastasis\n",
      "\n",
      "Topic  2\n",
      "valve, figure, echocardiography, heart, artery, mm, pressure, failure, atrium, vein\n",
      "\n",
      "Topic  3\n",
      "lung, day, chest, fig, treatment, hospital, tuberculosis, therapy, effusion, dyspnea\n",
      "\n",
      "Topic  4\n",
      "age, month, eye, week, seizure, rash, parent, mri, muscle, brain\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:289: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "nmf_model = NMF(5)\n",
    "# Learn an NMF model for given Document Term Matrix 'V' \n",
    "# Extract the document-topic matrix 'W'\n",
    "doc_topic = nmf_model.fit_transform(data_dtm_noun)\n",
    "# Extract top words from the topic-term matrix 'H' \n",
    "display_topics(nmf_model, tv_noun.get_feature_names_out(), 10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "45b4a809",
   "metadata": {},
   "source": [
    "For inspiration of visualizations see https://scikit-learn.org/stable/auto_examples/applications/plot_topics_extraction_with_nmf_lda.html"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "420ccc49",
   "metadata": {},
   "source": [
    "# Visualization and Analytical Methods:\n",
    "\n",
    "### Word Clouds:\n",
    "A tag cloud (also known as a word cloud, wordle or weighted list in visual design) is a visual representation of text data which is often used to depict keyword metadata on websites, or to visualize free form text. Tags are usually single words, and the importance of each tag is shown with font size or color. When used as website navigation aids, the terms are hyperlinked to items associated with the tag. [Wiki]\n",
    "\n",
    "https://en.wikipedia.org/wiki/Tag_cloud\n",
    "\n",
    "In this study, I will use wordcloud library\n",
    "\n",
    "https://pypi.org/project/wordcloud/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d6d242a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dictionary_maker(keys, values, order=1, slice_number=False):\n",
    "\n",
    "    # making the dictionary of words with their magnitude in each topic\n",
    "    dictionary = {}\n",
    "    for counter, key in enumerate(keys):\n",
    "        dictionary[key] = values[counter]\n",
    "    \n",
    "    # sort the dictionary in an ascending or descending order\n",
    "    dictionary = dict(sorted(dictionary.items(), key=lambda item: order * item[1]))\n",
    "\n",
    "    # slice the dictionary if needed\n",
    "    if slice_number != False:\n",
    "        dictionary = dict(itertools.islice(dictionary.items(), slice_number))\n",
    "    \n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1b28fc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def worldcloud_plot_maker(df, word_list, title_list=[], order=-1, row_column=0, color_map='magma'):\n",
    "    # sketch the main plot\n",
    "    fig = plt.figure(figsize=(10, 18.6))\n",
    "    plot_number = df.shape[row_column]\n",
    "\n",
    "    for number in range(plot_number):\n",
    "        # sketch the sub-plot\n",
    "        fig.add_subplot(plot_number,1,number+1)\n",
    "\n",
    "        if len(title_list) == 0:\n",
    "            plt.title(f'topic{number+1}', fontsize=20)        \n",
    "        else:\n",
    "            plt.title(title_list[number],fontsize=20)\n",
    "\n",
    "        if row_column == 0:\n",
    "            dictionary = dictionary_maker(word_list, df[number, :], order=order)\n",
    "        else:\n",
    "            dictionary = dictionary_maker(word_list, df[:, number], order=order)\n",
    "\n",
    "        wordcloud = WordCloud(background_color='white',  colormap=color_map)\n",
    "        wordcloud.generate_from_frequencies(dictionary) \n",
    "        \n",
    "        plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "\n",
    "        plt.axis(\"off\")\n",
    "    \n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4440c41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "worldcloud_plot_maker(nmf_model.components_, tv_noun.get_feature_names_out())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
