{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Portfolio Assignment week 02\n",
    "\n",
    "This week's focus is on manifold learning and text clustering. As part of the portfolio assignment, you are required to make a contribution to either the manifold learning case or the text clustering case. There are several options for your contribution, so you can choose the one that aligns with your learning style or interests the most\n",
    "\n",
    "\n",
    "### Manifold learning\n",
    "\n",
    "Study the Tutorial tutorial_manifold_tSNE and the tutorial_manifold_spectral_clustering and the Study_Case_pipeline. Next improve the code by comparing the performance of k-means and spectral clustering. Also compare PCA and t-SNE in the visualization of the result. You can use the pipeline function of scikit-learn and hyperparameter tuning with GridSearchCV. Here's a possible approach:\n",
    "\n",
    "- Load the dataset to be used for the clustering analysis.\n",
    "- Preprocess the dataset as needed (e.g., scale the features, normalize the data, etc.).\n",
    "- Define a pipeline with preprocessing and clustering\n",
    "- use PCA and t-SNE for dimension reduction and visualize the dimensions, use the clusters to color the datapoints\n",
    "- use GridSearchCV to optimize the hyper parameters\n",
    "- Evaluate the performance of the models using a suitable metric\n",
    "- choose the best cluster method and the best visualization method combination\n",
    "\n",
    "Explain choises and evaluate outcome. You can do this assignment in pairs but if you do so mention each others name. Do not forget to reference. If you cannot figure out how to use GridSearchCV and or a pipeline, use your own solution\n",
    "\n",
    "\n",
    "### Text clustering\n",
    "\n",
    "Read, execute and analyse the code in the notebook tutorial_clustering_words. Then *choose one* of the assignments a), b) or c). \n",
    "\n",
    "a) read the article Clinical Documents Clustering Based on Medication/Symptom Names Using Multi-View Nonnegative Matrix Factorization. you can find the article <a href = 'https://pubmed.ncbi.nlm.nih.gov/26011887/'> here</a>. Explain the similarities of this notebook and the article. Explain in your own words what need to be added to this notebook to reproduce the article. There is no need to code the solution, you can mention in your own words the steps. \n",
    "\n",
    "b) Improve the outcome improving the data preprocessing and the hyper parameter configurations. Explain your choices. Your solution should be a coded solution with comments. Are there any other weighting solutions next to TF-IDF?\n",
    "\n",
    "c) Provide a text clustering solution with your own data of interest, you can follow a similar approach to the one in the tutorial_clustering_words notebook. \n",
    "\n",
    "Mind you that you are not allowed to copy code solutions without referencing."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### some relevant links:\n",
    "word clustering: https://towardsdatascience.com/a-friendly-introduction-to-text-clustering-fa996bcefd04\n",
    "\n",
    "word embedding: https://towardsdatascience.com/introduction-to-word-embeddings-4cf857b12edc\n",
    "\n",
    "NMF: https://towardsdatascience.com/nmf-a-visual-explainer-and-python-implementation-7ecdd73491f8\n",
    "\n",
    "Stemming and lemmatization: https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html\n",
    "\n",
    "‘pos’ (part of speech) tagging: https://pythonprogramming.net/natural-language-toolkit-nltk-part-speech-tagging/\n",
    "\n",
    "TF-IDF: https://towardsdatascience.com/tf-idf-a-visual-explainer-and-python-implementation-on-presidential-inauguration-speeches-2a7671168550\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment \n",
    "the assignment that I choose to work on for this assignment is text clustering C.\n",
    "\n",
    "Provide a text clustering solution with your own data of interest, you can follow a similar approach to the one in the tutorial_clustering_words notebook. \n",
    "\n",
    "### Some extra terms :)\n",
    "\n",
    "When I start to study for this assignment, I thought that text clustering is a boring topic in machine learning. We need to clean a dataset, then implement some techniques to extract a group of topics. However, when I dove deeply into this topic, I found it quite amusing and most importantly informative. Now I am in the situation that I do not know what dataset should I choose since I am quite curious to study about all the Kaggle datasets :). At the end I came up with a fascinating data set about the abstracts of articles in Quantum Physics that gathers all the abstarct of 72880 articles related to different section of this amusing field of science. Consequently, as a datascienctis-physicist, I rather to choose to work on this dataset to find what topics (fields) are hot-topics during 1992-2020 in the field of quantum mechanics.\n",
    "\n",
    "dataset link: https://www.kaggle.com/datasets/louise2001/quantum-physics-articles-on-arxiv-1994-to-2009/discussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "# General libraries\n",
    "import re\n",
    "import os\n",
    "import string\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import yaml\n",
    "\n",
    "# word clustering libraries\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.feature_extraction import text\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from pathlib import Path\n",
    "\n",
    "# Analyzing and Visualizing modules\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "from tmtoolkit.topicmod.evaluate import metric_coherence_gensim"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Inspecting \n",
    "In this part first the configue file will be red, then some data inspection will be implemented on the dataset to acquire information about its differnet aspects. Finaly, the required attributes will be extracted and saved in a new data set for further investigation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inspired by https://fennaf.gitbook.io/bfvm22prog1/data-processing/configuration-files/yaml\n",
    "\n",
    "def configReader():\n",
    "    \"\"\"\n",
    "    explanation: This function open config,yaml file \n",
    "    and fetch the gonfigue file information\n",
    "    input: ...\n",
    "    output: configue file\n",
    "    \"\"\"\n",
    "    with open(\"config.yaml\", \"r\") as inputFile:\n",
    "        config = yaml.safe_load(inputFile)\n",
    "    return config\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>categories</th>\n",
       "      <th>created</th>\n",
       "      <th>id</th>\n",
       "      <th>doi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>General System theory, Like-Quantum Semantics ...</td>\n",
       "      <td>It is outlined the possibility to extend the q...</td>\n",
       "      <td>['physics.gen-ph', 'quant-ph']</td>\n",
       "      <td>2007-03-31</td>\n",
       "      <td>0704.0042</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Entanglement of Subspaces and Error Correcting...</td>\n",
       "      <td>We introduce the notion of entanglement of sub...</td>\n",
       "      <td>['quant-ph']</td>\n",
       "      <td>2007-04-02</td>\n",
       "      <td>0704.0251</td>\n",
       "      <td>10.1103/PhysRevA.76.042309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>General sequential quantum cloning</td>\n",
       "      <td>Some multipartite quantum states can be genera...</td>\n",
       "      <td>['quant-ph']</td>\n",
       "      <td>2007-04-03</td>\n",
       "      <td>0704.0323</td>\n",
       "      <td>10.1088/1751-8113/41/15/155303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Levy-Lieb constrained-search formulation as a ...</td>\n",
       "      <td>The constrained-search formulation of Levy and...</td>\n",
       "      <td>['quant-ph']</td>\n",
       "      <td>2007-04-03</td>\n",
       "      <td>0704.0372</td>\n",
       "      <td>10.1088/1751-8113/40/11/013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Review: Semiconductor Quantum Light Sources</td>\n",
       "      <td>Lasers and LEDs display a statistical distribu...</td>\n",
       "      <td>['quant-ph']</td>\n",
       "      <td>2007-04-03</td>\n",
       "      <td>0704.0403</td>\n",
       "      <td>10.1038/nphoton.2007.46</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  General System theory, Like-Quantum Semantics ...   \n",
       "1  Entanglement of Subspaces and Error Correcting...   \n",
       "2                 General sequential quantum cloning   \n",
       "3  Levy-Lieb constrained-search formulation as a ...   \n",
       "4        Review: Semiconductor Quantum Light Sources   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  It is outlined the possibility to extend the q...   \n",
       "1  We introduce the notion of entanglement of sub...   \n",
       "2  Some multipartite quantum states can be genera...   \n",
       "3  The constrained-search formulation of Levy and...   \n",
       "4  Lasers and LEDs display a statistical distribu...   \n",
       "\n",
       "                       categories     created         id  \\\n",
       "0  ['physics.gen-ph', 'quant-ph']  2007-03-31  0704.0042   \n",
       "1                    ['quant-ph']  2007-04-02  0704.0251   \n",
       "2                    ['quant-ph']  2007-04-03  0704.0323   \n",
       "3                    ['quant-ph']  2007-04-03  0704.0372   \n",
       "4                    ['quant-ph']  2007-04-03  0704.0403   \n",
       "\n",
       "                              doi  \n",
       "0                             NaN  \n",
       "1      10.1103/PhysRevA.76.042309  \n",
       "2  10.1088/1751-8113/41/15/155303  \n",
       "3     10.1088/1751-8113/40/11/013  \n",
       "4         10.1038/nphoton.2007.46  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def dataframe_maker(config):\n",
    "    file_directory, file_name = config.values()\n",
    "    os.chdir(file_directory)\n",
    "    df = pd.read_csv(file_name)\n",
    "    return df\n",
    "df = dataframe_maker(configReader())\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset contains six topics (title, absttact, categories, created, id, doi). also it contains 72880 articles (samples). in the next part some data inspection will perfome on the dataset to gain more information about the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of null value in each column is:\n",
      "title             0\n",
      "abstract          0\n",
      "categories        0\n",
      "created           0\n",
      "id                0\n",
      "doi           21051\n",
      "dtype: int64\n",
      "\n",
      "number of different categories: 4174\n",
      "however most of these topics are a mixture of basic topics\n",
      "\n",
      "this dataset ranges from 1992 to 2020\n",
      "\n",
      "total number of ids is 72881\n",
      "the number of unique ids is 72881\n"
     ]
    }
   ],
   "source": [
    "# make a data inspection function\n",
    "def data_inspector(df):\n",
    "    # find the number of null values in the dataset.\n",
    "    null_values = df.isnull().sum()\n",
    "    print(f'the number of null value in each column is:\\n{null_values}\\n')\n",
    "\n",
    "    # find the number of categories in the dataset\n",
    "    categories_list = df.categories.unique()\n",
    "    print(f'number of different categories: {len(categories_list)}')\n",
    "    print('however most of these topics are a mixture of basic topics\\n')\n",
    "\n",
    "    # find the time period for this publishing these articles\n",
    "    date_list = pd.to_datetime(df.created)\n",
    "    print(f'this dataset ranges from {date_list.dt.year.min()} to {date_list.dt.year.max()}\\n')\n",
    "\n",
    "    # find whether an id is assigned to more than one article\n",
    "    unique_ids = df.id.unique()\n",
    "    print(f'total number of ids is {len(df.id)}\\nthe number of unique ids is {len(unique_ids)}')\n",
    "\n",
    "data_inspector(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the data inspection part, one can infer that the best attribute for assigning as index is id since first doi contains many null values, also there are unique id for each article.\n",
    "\n",
    "One can decide to work on the abstracts or on the titles. I personally think abstracts are more informative since they contain more information. Consequently in the next step I will make a series that contains all the abstracts as the values and ids as their index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0704.0042</th>\n",
       "      <td>It is outlined the possibility to extend the q...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0704.0251</th>\n",
       "      <td>We introduce the notion of entanglement of sub...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0704.0323</th>\n",
       "      <td>Some multipartite quantum states can be genera...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0704.0372</th>\n",
       "      <td>The constrained-search formulation of Levy and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0704.0403</th>\n",
       "      <td>Lasers and LEDs display a statistical distribu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    abstract\n",
       "id                                                          \n",
       "0704.0042  It is outlined the possibility to extend the q...\n",
       "0704.0251  We introduce the notion of entanglement of sub...\n",
       "0704.0323  Some multipartite quantum states can be genera...\n",
       "0704.0372  The constrained-search formulation of Levy and...\n",
       "0704.0403  Lasers and LEDs display a statistical distribu..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstract_df = df[['id', 'abstract']]\n",
    "abstract_df = abstract_df.set_index('id')\n",
    "abstract_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Texts\n",
    "In this section, I will utilize the function featured and designed in the link provided below. Moreover, I will focus solely on the nouns as the research question aims to identify the hot topics in Quantum Mechanics over the past three decades. Therefore, the essential terms will consist of nouns encompassing subjects, techniques, particle names, and similar concepts.\n",
    "\n",
    "https://towardsdatascience.com/nmf-a-visual-explainer-and-python-implementation-7ecdd73491f8\n",
    "\n",
    "Also, one can find more information about the following functions and also the modules that are used inside them in the tutorial_clustering_words.ipynb file in this folder or on the following link.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    '''Make text lowercase, remove text in square brackets, \n",
    "    remove punctuation, remove read errors,\n",
    "    and remove words containing numbers.'''\n",
    "    text = text.lower()\n",
    "    # replace anything that exists in a square bracket with space\n",
    "    text = re.sub('\\[.*?\\]', ' ', text)\n",
    "    # %s means its a string, %d is an integer, %f is floating point number.\n",
    "    # escape is a function that escapes any special characters present in the the string.\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), ' ', text)\n",
    "    # '\\w*\\d\\w*' is a regular expression pattern that matches any word that contains at least one digit.\n",
    "    text = re.sub('\\w*\\d\\w*', ' ', text)\n",
    "    # replace the errors with space\n",
    "    text = re.sub('�', ' ', text)\n",
    "    return text\n",
    "\n",
    "cleaned = lambda x: clean_text(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noun extract and lemmatize function\n",
    "def nouns(text):\n",
    "    '''Given a string of text, tokenize the text \n",
    "    and pull out only the nouns.'''\n",
    "    # create mask to isolate words that are nouns\n",
    "    is_noun = lambda pos: pos[:2] == 'NN'\n",
    "    # store function to split string of words \n",
    "    # into a list of words (tokens)\n",
    "    tokenized = word_tokenize(text)\n",
    "    # store function to lemmatize each word\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    # use list comprehension to lemmatize all words \n",
    "    # and create a list of all nouns\n",
    "    all_nouns = [wordnet_lemmatizer.lemmatize(word) \\\n",
    "    for (word, pos) in pos_tag(tokenized) if is_noun(pos)] \n",
    "    \n",
    "    #return string of joined list of nouns\n",
    "    return ' '.join(all_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0704.0042</th>\n",
       "      <td>possibility quantum formalism relation require...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0704.0251</th>\n",
       "      <td>notion entanglement subspace measure entanglem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0704.0323</th>\n",
       "      <td>state manner setup microwave cavity qed ion do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0704.0372</th>\n",
       "      <td>search formulation levy lieb hohenberg electro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0704.0403</th>\n",
       "      <td>laser led distribution number photon time appl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    abstract\n",
       "id                                                          \n",
       "0704.0042  possibility quantum formalism relation require...\n",
       "0704.0251  notion entanglement subspace measure entanglem...\n",
       "0704.0323  state manner setup microwave cavity qed ion do...\n",
       "0704.0372  search formulation levy lieb hohenberg electro...\n",
       "0704.0403  laser led distribution number photon time appl..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean Text\n",
    "abstract_df[\"abstract\"] = abstract_df[\"abstract\"].apply(cleaned)\n",
    "data_nouns = pd.DataFrame(abstract_df[\"abstract\"].apply(nouns))\n",
    "# Visually Inspect\n",
    "data_nouns.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage the data has cleaned and all the nouns has been extracted from the abstract. Consequently, the corpus is ready for further investigation. In the next step the Document-Term matrix (DTM) will be extracted using TF-IDF (Term Frequency-Inverse Document Frequency) method. For further information about TF-IDF and also TfidfVectorizer Sklearn class one can study tutorial_clustering_words.ipynb.\n",
    "\n",
    "## The Document-Term Matrix (DTM)\n",
    "In this stage one needs to extract DTM to use it as the input of NMF method. Again I will use the funtions introduced in the tutorial notebook. Also I added some documentation and explanations to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ability</th>\n",
       "      <th>absence</th>\n",
       "      <th>absorption</th>\n",
       "      <th>access</th>\n",
       "      <th>account</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>action</th>\n",
       "      <th>addition</th>\n",
       "      <th>advantage</th>\n",
       "      <th>agreement</th>\n",
       "      <th>...</th>\n",
       "      <th>waveguide</th>\n",
       "      <th>wavelength</th>\n",
       "      <th>way</th>\n",
       "      <th>width</th>\n",
       "      <th>wigner</th>\n",
       "      <th>work</th>\n",
       "      <th>world</th>\n",
       "      <th>year</th>\n",
       "      <th>yield</th>\n",
       "      <th>zero</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.269073</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.19872</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 539 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ability  absence  absorption  access   account  accuracy  action  addition  \\\n",
       "0      0.0      0.0         0.0     0.0  0.269073       0.0     0.0       0.0   \n",
       "1      0.0      0.0         0.0     0.0  0.000000       0.0     0.0       0.0   \n",
       "2      0.0      0.0         0.0     0.0  0.000000       0.0     0.0       0.0   \n",
       "3      0.0      0.0         0.0     0.0  0.000000       0.0     0.0       0.0   \n",
       "4      0.0      0.0         0.0     0.0  0.000000       0.0     0.0       0.0   \n",
       "\n",
       "   advantage  agreement  ...  waveguide  wavelength  way  width  wigner  work  \\\n",
       "0        0.0        0.0  ...        0.0         0.0  0.0    0.0     0.0   0.0   \n",
       "1        0.0        0.0  ...        0.0         0.0  0.0    0.0     0.0   0.0   \n",
       "2        0.0        0.0  ...        0.0         0.0  0.0    0.0     0.0   0.0   \n",
       "3        0.0        0.0  ...        0.0         0.0  0.0    0.0     0.0   0.0   \n",
       "4        0.0        0.0  ...        0.0         0.0  0.0    0.0     0.0   0.0   \n",
       "\n",
       "   world     year  yield  zero  \n",
       "0    0.0  0.00000    0.0   0.0  \n",
       "1    0.0  0.00000    0.0   0.0  \n",
       "2    0.0  0.00000    0.0   0.0  \n",
       "3    0.0  0.00000    0.0   0.0  \n",
       "4    0.0  0.19872    0.0   0.0  \n",
       "\n",
       "[5 rows x 539 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def dtm_maker(data_nouns):\n",
    "    # Create a document-term matrix with only nouns\n",
    "    # Store TF-IDF Vectorizer\n",
    "    tv_noun = TfidfVectorizer(stop_words=text.ENGLISH_STOP_WORDS, ngram_range = (1,1), max_df = .8, min_df = .01)\n",
    "    # Fit and Transform speech noun text to a TF-IDF Doc-Term Matrix\n",
    "    data_tv_noun = tv_noun.fit_transform(data_nouns.abstract)\n",
    "    # Create data-frame of Doc-Term Matrix with nouns as column names\n",
    "    # first it changes the data_tv_noun to an array and then use the nouns exist in tv_noun as the columns of the dataset\n",
    "    data_dtm_noun = pd.DataFrame(data_tv_noun.toarray(), columns=tv_noun.get_feature_names_out())\n",
    "    data_dtm_noun.index = df.index\n",
    "    # Visually inspect Document Term Matrix\n",
    "    return tv_noun, data_dtm_noun\n",
    "\n",
    "tv_noun, data_dtm_noun = dtm_maker(data_nouns)\n",
    "data_dtm_noun.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By utilizing TfidfVectorizer module, 539 most important terms of this corpus come to existance. In the next step one can implement NMF Sklearn module to make document-topic and topic-term matrices. Again for more information about the functions one can study the tutorial part.\n",
    "\n",
    "## NMF\n",
    "I made a small function to make an NMF object, then use it in the display_topic() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nmf_maker(df, topic_number):\n",
    "    nmf_model = NMF(topic_number)\n",
    "    doc_topic = nmf_model.fit_transform(df)\n",
    "    return nmf_model, doc_topic\n",
    "\n",
    "def display_topics(model, feature_names, num_top_words, topic_names=None):\n",
    "    '''Given an NMF model, feature_names, and number of top words, print \n",
    "       topic number and its top feature names, up to specified number of top words.'''\n",
    "    # iterate through topics in topic-term matrix, 'H' aka\n",
    "    # model.components_\n",
    "    for ix, topic in enumerate(model.components_):\n",
    "        #print topic, topic number, and top words\n",
    "        # if there is no topic or term print\n",
    "        if not topic_names or not topic_names[ix]:\n",
    "            print(\"\\nTopic \", ix)\n",
    "        # print topics and their most frequent words\n",
    "        else:\n",
    "            print(\"\\nTopic: '\",topic_names[ix],\"'\")\n",
    "        print(\", \".join([feature_names[i] \\\n",
    "             for i in topic.argsort()[:-num_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:289: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn(\n",
      "c:\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1637: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  0\n",
      "quantum, protocol, measurement, information, channel, qubit, algorithm, error, gate, qubits\n",
      "\n",
      "Topic  1\n",
      "time, equation, theory, particle, function, model, energy, space, quantum, field\n",
      "\n",
      "Topic  2\n",
      "photon, cavity, mode, atom, field, frequency, source, light, laser, emission\n",
      "\n",
      "Topic  3\n",
      "state, entanglement, measurement, correlation, measure, entropy, inequality, qubit, ground, bound\n",
      "\n",
      "Topic  4\n",
      "spin, phase, field, interaction, transition, model, electron, chain, atom, temperature\n"
     ]
    }
   ],
   "source": [
    "nmf_model, doc_topic = nmf_maker(data_dtm_noun, 5)\n",
    "display_topics(nmf_model, tv_noun.get_feature_names_out(), 10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I explored a range of topics, varying from 5 to 20, within the corpus. Through this experimentation, I discovered that extracting five topics successfully captured the five primary subcategories of Quantum Physics. As the number of topics increased, these major subjects further fragmented into their respective sub-parts. This fragmentation remains valid, considering that each major subtopic comprises numerous distinct components. However, for the purpose of this limited study, isolating the primary subject of quantum mechanics should suffice. Therefore, I will proceed with these five major topics, assigning them names and creating a corresponding dictionary. Subsequently, I will employ various analysis and visualization techniques to further investigate them."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
