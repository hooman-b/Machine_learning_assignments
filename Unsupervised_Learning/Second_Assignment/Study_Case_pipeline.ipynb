{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff4e9474",
   "metadata": {},
   "source": [
    "# Demo notebook for a pipeline functions\n",
    "\n",
    "This demonstration notebook demonstrates the usage of a pipeline function. See also Scikit-learn's [pipeline function](http://scikit-learn.org/stable/modules/pipeline.html). The sklearn pipeline function is a tool in machine learning that simplifies the workflow by encapsulating all the steps involved in a single object. It offers advantages such as simplicity, reproducibility, efficiency, flexibility, and integration. \n",
    "\n",
    "The Pipeline is built using a list of (key, value) pairs, where the key is a string containing the name you want to give this step and value is an estimator object (the method to be executed). \n",
    "\n",
    "This notebook demonstrates several use cases, comparing the single steps to the pipeline step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00e0da28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import time\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import GridSearchCV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e00da7",
   "metadata": {},
   "source": [
    "## The data\n",
    "\n",
    "For this notebook we use lifelines data. Lifelines is a large, multi-generational, prospective cohort study that includes over 167,000 participants (10%) from the northern population of the Netherlands. Within this cohort study the participants are followed over a 30-year period. Every five years, participants visit one of the Lifelines sites in the northern parts of the Netherlands for an assessment. During these visits, several physical measurements are taken and different biomaterials are collected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab95e6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('lifelines.txt', sep = '\\t').set_index('ZIPCODE')\n",
    "#df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3776e7be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(480, 71)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f30bc4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a copy to compare manual way with pipeline way\n",
    "data_manual = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53abb90a",
   "metadata": {},
   "source": [
    "## Use case preprocessing\n",
    "\n",
    "First we will demonstrate the manual steps, the log tranformation and the scaling. Then we demonstrate the pipeline way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "305328c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0023238658905029297\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "# single step log transformation\n",
    "data_manual = np.log1p(data_manual)\n",
    "# single step scaling\n",
    "mms = MinMaxScaler()\n",
    "data_manual = mms.fit_transform(data_manual)\n",
    "print(time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9e1f0c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0028982162475585938\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "# The pipeline\n",
    "prep = Pipeline([('log1p', FunctionTransformer(np.log1p)), \n",
    "                 ('minmaxscale', MinMaxScaler())]\n",
    "               )\n",
    "\n",
    "# Convert the original data\n",
    "data_pipe = prep.fit_transform(df)\n",
    "print(time() - t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c93a375",
   "metadata": {},
   "source": [
    "Note that the `pipeline` uses the `fit_transform()` function. The MinMaxScaler does have the fit_transform function but the np.log1p function does not have this. We therefore use the sklearn `FunctionTransformer` function. The `FunctionTransformer` is a class in the sklearn.preprocessing module that allows you to apply an arbitrary function to your data. It is commonly used as a preprocessing step in sklearn pipelines.\n",
    "\n",
    "The FunctionTransformer constructor takes as input a callable, which can be any Python function or lambda expression. When applied to a dataset, the FunctionTransformer simply calls the specified function on the input data and returns the transformed output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d9811d",
   "metadata": {},
   "source": [
    "\n",
    "`np.allclose` is a function in the NumPy library that compares two arrays element-wise and returns True if all corresponding pairs of elements are within a specified tolerance of each other, and False otherwise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b2db22e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check if values are the same, pipeline works same as the manual steps\n",
    "np.allclose(data_pipe, data_manual)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e41a82",
   "metadata": {},
   "source": [
    "## Extend pipeline with PCA\n",
    "We can use the preprocessing pipeline in a new pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "714320c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_manual = df.copy()\n",
    "# single step log transformation\n",
    "data_manual = np.log1p(data_manual)\n",
    "# single step scaling\n",
    "mms = MinMaxScaler()\n",
    "data_manual = mms.fit_transform(data_manual)\n",
    "#single step PCA\n",
    "pca = PCA(2, random_state=42)\n",
    "result_manual_pca = pca.fit_transform(data_manual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4cf5c75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use case PCA\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('prep', prep),\n",
    "    ('pca', PCA(2, random_state=42))\n",
    "])\n",
    "\n",
    "results_pipe_pca = pipe.fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2034e59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(result_manual_pca, results_pipe_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf97cbf0",
   "metadata": {},
   "source": [
    "# Use a pipeline for clustering\n",
    "\n",
    "The same way we used the pipeline for performing preprocessing and PCA, we can make a pipeline for preprocessing and clustering with Kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6303ae58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 5, 1, 4, 4, 5, 5, 5, 4, 7, 1, 0, 4, 5, 0, 5, 5, 5, 1, 4, 5, 5,\n",
       "       7, 5, 4, 7, 0, 7, 5, 5, 5, 5, 4, 5, 5, 5, 7, 1, 1, 1, 1, 7, 1, 7,\n",
       "       3, 1, 3, 5, 3, 3, 7, 3, 3, 3, 3, 3, 5, 3, 0, 1, 0, 1, 7, 3, 3, 7,\n",
       "       4, 3, 3, 4, 4, 4, 7, 3, 7, 4, 4, 4, 4, 4, 4, 4, 7, 7, 3, 3, 5, 3,\n",
       "       7, 4, 0, 4, 3, 1, 3, 1, 3, 4, 3, 3, 1, 1, 1, 0, 1, 1, 0, 1, 0, 7,\n",
       "       0, 3, 3, 3, 0, 1, 4, 0, 4, 7, 7, 7, 0, 7, 7, 3, 4, 3, 7, 3, 3, 3,\n",
       "       3, 3, 3, 3, 0, 7, 1, 7, 7, 7, 1, 1, 7, 6, 3, 6, 6, 6, 7, 6, 6, 7,\n",
       "       6, 6, 6, 7, 0, 3, 3, 4, 3, 4, 3, 3, 0, 7, 4, 1, 4, 7, 3, 1, 0, 4,\n",
       "       3, 4, 1, 1, 7, 7, 7, 7, 7, 2, 0, 1, 1, 0, 5, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 0, 0, 4, 3, 3, 7, 6, 6, 0, 3, 0, 7, 7, 7, 3, 1, 3,\n",
       "       3, 0, 4, 3, 4, 4, 4, 0, 3, 4, 0, 0, 4, 4, 4, 4, 1, 1, 1, 1, 0, 1,\n",
       "       6, 0, 6, 4, 3, 3, 6, 6, 5, 1, 1, 3, 3, 6, 6, 3, 3, 3, 7, 4, 4, 3,\n",
       "       4, 3, 3, 3, 3, 3, 4, 1, 7, 4, 7, 4, 5, 4, 5, 3, 3, 3, 4, 3, 3, 3,\n",
       "       1, 0, 0, 7, 0, 7, 1, 4, 3, 1, 0, 7, 7, 7, 0, 1, 1, 1, 1, 1, 1, 0,\n",
       "       7, 7, 1, 1, 4, 0, 4, 5, 4, 7, 4, 7, 4, 4, 0, 0, 0, 3, 4, 4, 4, 7,\n",
       "       5, 1, 5, 4, 5, 5, 5, 4, 7, 5, 5, 5, 5, 5, 6, 5, 5, 4, 6, 7, 7, 7,\n",
       "       0, 7, 0, 7, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 5, 5, 1, 5, 1,\n",
       "       1, 5, 4, 5, 5, 5, 5, 0, 5, 5, 5, 7, 6, 5, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 0, 0, 1, 1, 0, 1, 1, 2, 1, 1, 1, 0, 0, 0, 0,\n",
       "       0, 7, 7, 0, 7, 0, 6, 7, 6, 6, 6, 3, 0, 7, 0, 3, 0, 7, 5, 4, 4, 3,\n",
       "       3, 3, 4, 3, 7, 1, 1, 0, 4, 4, 4, 4, 4, 6, 6, 7, 4, 5, 7, 5, 4, 4,\n",
       "       4, 4, 6, 1, 3, 6, 1, 4, 4, 1, 5, 4, 7, 7, 5, 1, 5, 5], dtype=int32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate the KMeans object\n",
    "from sklearn.pipeline import make_pipeline\n",
    "kmeans = KMeans()\n",
    "\n",
    "# Instantiate the Pipeline object with the StandardScaler and KMeans objects\n",
    "pipeline = Pipeline([\n",
    "    ('prep', prep),\n",
    "    ('kmeans', kmeans), \n",
    "])\n",
    "\n",
    "# Fit the pipeline to the data\n",
    "pipeline.fit_predict(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c473ff",
   "metadata": {},
   "source": [
    "## Optimze with gridsearchCV and pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6970989f",
   "metadata": {},
   "source": [
    "Most of the time pipeline is used in combination with `GridSearchCV`. \n",
    "`Pipeline` is often used in combination with `GridSearchCV` for hyperparameter tuning because it allows us to encapsulate all the preprocessing and modeling steps into a single object. This can be useful for several reasons:\n",
    "\n",
    "1. Convenience: Instead of manually performing each preprocessing step and modeling step separately, we can combine them into a single object and pass them as an argument to `GridSearchCV`. This can make our code simpler and easier to read.\n",
    "\n",
    "2. Reproducibility: By encapsulating all the preprocessing and modeling steps into a single object, we can ensure that the same steps are applied consistently to the data, regardless of how many times we run the code.\n",
    "\n",
    "3. Avoiding data leakage: When performing hyperparameter tuning, it's important to ensure that the evaluation of each parameter combination is done using only the training data, not the validation data. By including the preprocessing steps in the `Pipeline` object, we can ensure that the same preprocessing steps are applied to both the training and validation data, preventing data leakage.\n",
    "\n",
    "`GridSearchCV` is used to search over a grid of hyperparameters to find the best combination of hyperparameters that maximizes some performance metric, see `sklearn.metrics`. By using `Pipeline` in combination with `GridSearchCV`, we can search over a grid of hyperparameters for both the preprocessing and modeling steps simultaneously. This can be a powerful technique for optimizing the performance of machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30e0fa2",
   "metadata": {},
   "source": [
    "We can configure `GridSearchCV` with: \n",
    "- estimator: estimator object being used, can be the result of a pipeline\n",
    "- param_grid: dictionary that contains all of the parameters to try\n",
    "- scoring: evaluation metric to use when ranking results\n",
    "- cv: cross-validation, the number of cv folds for each combination of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2de6062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'kmeans__init': 'random', 'kmeans__n_clusters': 6, 'kmeans__random_state': 42}\n",
      "-70.93532143198493\n"
     ]
    }
   ],
   "source": [
    "hyperparams = {\n",
    "    \"kmeans__n_clusters\": [2, 3, 4, 5, 6],\n",
    "    \"kmeans__random_state\": [42],\n",
    "    \"kmeans__init\": ['k-means++', 'random']\n",
    "}\n",
    "\n",
    "# Instantiate the KMeans object\n",
    "kmeans = KMeans()\n",
    "\n",
    "# Instantiate the Pipeline object with the StandardScaler and KMeans objects\n",
    "pipeline = Pipeline([\n",
    "    ('prep', prep),\n",
    "    ('kmeans', kmeans)\n",
    "])\n",
    "\n",
    "# Instantiate the GridSearchCV object with the Pipeline object and parameter grid\n",
    "grid_search = GridSearchCV(estimator=pipeline, param_grid=hyperparams, cv=10, n_jobs=-1)\n",
    "\n",
    "# Fit the GridSearchCV object to the data\n",
    "grid_search.fit(df)\n",
    "\n",
    "# Access the best combination of hyperparameters and the corresponding evaluation metric score\n",
    "print(grid_search.best_params_)\n",
    "print(grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6949ffe3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsls",
   "language": "python",
   "name": "dsls"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
