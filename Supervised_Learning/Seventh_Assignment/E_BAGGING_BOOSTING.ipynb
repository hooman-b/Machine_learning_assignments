{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "553656ef",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "In this notebook we will cover the following topics:\n",
    "- Bagging\n",
    "- Boosting\n",
    "\n",
    "These techniques are related and are crucial to understand as they are valuable in a plethora of circumstances. They are based on the idea that a combination of multiple models produce a more powerful estimator. They are ensemble methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f3c63993",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30906a14",
   "metadata": {},
   "source": [
    "# Ensemble methods\n",
    "Ensemble models are based on the idea that combining several 'weak' models result in a 'strong' model by combining the predictions. \n",
    "\n",
    "An example would be combining several decision trees and take the average prediction. If there are 100 trees, and 60 trees predict *yes*, then the final model will predict *yes*.\n",
    "\n",
    "Given a complex dataset and a base model (weak learner) we can train the model on the dataset. Often, a single model does not perform well, because it has either a high bias or high variance. The main idea is to try reducing the bias or variance of the base models by combining multiple base models to create an ensemble model that achieves better performance.\n",
    "\n",
    "For a refresher of the bias-variance tradeoff, you can read the <a href=\"https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff\" target=\"_blank\">wiki</a>\n",
    "\n",
    "In general, there are three major algorithms that can combine multiple base models:\n",
    "- Bagging: learns homogeneous base models independently in parallel and combines them through an averaging process.\n",
    "- Boosting: learns homogeneous base models sequentially.\n",
    "- Stacking: learns heterogenous base models in parallel and combines them by training a meta-model to output a prediction.\n",
    "\n",
    "A visualization of the idea (<a href=\"https://towardsdatascience.com/ensemble-methods-bagging-boosting-and-stacking-c9214a10a205\">src</a>)\n",
    "\n",
    "![img](../Images/ensemble.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdaa7ae",
   "metadata": {},
   "source": [
    "# Bagging (Bootstrap Aggregating)\n",
    "Bagging stands for bootstrap aggregating and consists of training several models in parallel with replacement. Let's get into boostrapping first.\n",
    "\n",
    "## Bootstrapping\n",
    "Given a dataset $N$, generate bootstrap samples of size $B$ by randomly drawing with replacement $B$ observations.\n",
    "\n",
    "$$N= \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{bmatrix}$$\n",
    "\n",
    "Given $B = 3$\n",
    "\n",
    "$$\\text{Bootstrap samples} = \\begin{bmatrix} 1 & 3 & 8 \\\\ 7 & 6 & 6 \\\\ ... \\\\ 9 & 3 & 2 \\end{bmatrix}$$\n",
    "\n",
    "Makes the approximation that data is being drawn from the true underlying distribution and independently from each other.\n",
    "\n",
    "This is useful to measure the variance in a dataset.\n",
    "\n",
    "## Bagging\n",
    "When training a model, this model is subject to variability. This means that if it was trained on another dataset, we would have a different model. Therefore, it is helpful to create several independent models and average their predictions. By averaging the base models, we obtain a final model with less variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c3e0a249",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples=100, n_features=4,\n",
    "                          n_informative=2, n_redundant=2,\n",
    "                          random_state=0, shuffle=False)\n",
    "clf = BaggingClassifier(estimator=SVC(), n_estimators=10, random_state=0)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9690e078",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-7 {color: black;background-color: white;}#sk-container-id-7 pre{padding: 0;}#sk-container-id-7 div.sk-toggleable {background-color: white;}#sk-container-id-7 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-7 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-7 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-7 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-7 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-7 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-7 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-7 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-7 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-7 div.sk-item {position: relative;z-index: 1;}#sk-container-id-7 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-7 div.sk-item::before, #sk-container-id-7 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-7 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-7 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-7 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-7 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-7 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-7 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-7 div.sk-label-container {text-align: center;}#sk-container-id-7 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-7 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>BaggingClassifier(estimator=SVC(), random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-19\" type=\"checkbox\" ><label for=\"sk-estimator-id-19\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">BaggingClassifier</label><div class=\"sk-toggleable__content\"><pre>BaggingClassifier(estimator=SVC(), random_state=0)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-20\" type=\"checkbox\" ><label for=\"sk-estimator-id-20\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-21\" type=\"checkbox\" ><label for=\"sk-estimator-id-21\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "BaggingClassifier(estimator=SVC(), random_state=0)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5f984664",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bedc2627",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5405892f",
   "metadata": {},
   "source": [
    "# Boosting\n",
    "With boosting, models are fitted iteratively, thus sequentially. It produces a model that is in general less biased.\n",
    "\n",
    "Given a sequence of models, each new model gives more importance to observations in the dataset that were badly handled. Thus, the weights are adjusted.\n",
    "\n",
    "A visualization of the idea (<a href=\"https://towardsdatascience.com/ensemble-methods-bagging-boosting-and-stacking-c9214a10a205\">src</a>)\n",
    "\n",
    "![img](../Images/boosting.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "232b94ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = AdaBoostClassifier(n_estimators=100, random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "pred = clf.predict(X_test)\n",
    "accuracy_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252f2f34",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Every type of model has its own unique properties that work best with specific kinds of data. Bagging is often used to reduce variance. This can be very handy for noisy data. Boosting is often used to reduce bias. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e810215c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
