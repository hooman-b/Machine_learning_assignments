{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Portfolio assignment week 7\n",
    "\n",
    "## 1. Bagging vs Boosting\n",
    "The scikit-learn library provides several options for bagging and boosting. It is possible to create your own boosting model based on a base model. For instance, you can create a tree based bagging model. In addition, scikit-learn provides AdaBoost. For XGBoost it is best to use the xgboost library.\n",
    "\n",
    "Based on the theory in the [accompanying notebook](../Exercises/E_BAGGING_BOOSTING.ipynb), create a bagging, boosting and dummy classifier. Test these classifiers on the [breast cancer dataset](https://www.kaggle.com/datasets/yasserh/breast-cancer-dataset). Go through the data science pipeline as you've done before:\n",
    "\n",
    "1. Try to understand the dataset globally.\n",
    "2. Load the data.\n",
    "3. Exploratory analysis\n",
    "4. Preprocess data (skewness, normality, etc.)\n",
    "5. Modeling (cross-validation and training). (**Create several bagging classifiers with different estimators**.)\n",
    "6. Evaluation (**Use the evaluation methods as described in the previous lessons. Then compare the different models**.)\n",
    "7. Try to understand why some methods perform better than others. Try different configurations for your bagging and boosting models.\n",
    "\n",
    "# Dataset\n",
    "While I should admit that using the same dataset twice is not fun at all, the outcome of the privious Assignment (Assignment6) urges to use the Titanic dataset again for this assignment. The reason is that the model generated through the Decision Tree approach did not exhibit satisfactory accuracy, achieving only 80% accuracy in label predictions.Consequently, I am inclined to explore the application of a meta-model to the dataset. The aim is to potentially yield improved outcomes—a more suitable model—for accurately predicting the labels.\n",
    "\n",
    "Consequently, in this assignment I will use The <a href=\"https://www.kaggle.com/datasets/vinicius150987/titanic3?resource=download\" target=\"_blank\">Titanic</a> dataset again and try to improve the predictiong power by using different meta models. For more information about this dataset one can delve into details of the privious assignment.\n",
    "\n",
    "# Data Loading\n",
    "First, I import the relavant libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Modules\n",
    "import yaml\n",
    "import os\n",
    "import graphviz\n",
    "import time\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# Data generating and feature modules\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_moons, make_circles\n",
    "\n",
    "# preprocessing modules\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Classification modules\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn import tree\n",
    "\n",
    "# meta-Model modules\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Evaluation modules\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inspired by https://fennaf.gitbook.io/bfvm22prog1/data-processing/configuration-files/yaml\n",
    "\n",
    "def configReader():\n",
    "    \"\"\"\n",
    "    explanation: This function open config,yaml file \n",
    "    and fetch the gonfigue file information\n",
    "    input: ...\n",
    "    output: configue file\n",
    "    \"\"\"\n",
    "    with open(\"config.yaml\", \"r\") as inputFile:\n",
    "        config = yaml.safe_load(inputFile)\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pclass</th>\n",
       "      <th>survived</th>\n",
       "      <th>name</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>ticket</th>\n",
       "      <th>fare</th>\n",
       "      <th>cabin</th>\n",
       "      <th>embarked</th>\n",
       "      <th>boat</th>\n",
       "      <th>body</th>\n",
       "      <th>home.dest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Allen, Miss. Elisabeth Walton</td>\n",
       "      <td>female</td>\n",
       "      <td>29.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>24160</td>\n",
       "      <td>211.3375</td>\n",
       "      <td>B5</td>\n",
       "      <td>S</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>St Louis, MO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Allison, Master. Hudson Trevor</td>\n",
       "      <td>male</td>\n",
       "      <td>0.9167</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>113781</td>\n",
       "      <td>151.5500</td>\n",
       "      <td>C22 C26</td>\n",
       "      <td>S</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Montreal, PQ / Chesterville, ON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Allison, Miss. Helen Loraine</td>\n",
       "      <td>female</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>113781</td>\n",
       "      <td>151.5500</td>\n",
       "      <td>C22 C26</td>\n",
       "      <td>S</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Montreal, PQ / Chesterville, ON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Allison, Mr. Hudson Joshua Creighton</td>\n",
       "      <td>male</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>113781</td>\n",
       "      <td>151.5500</td>\n",
       "      <td>C22 C26</td>\n",
       "      <td>S</td>\n",
       "      <td>NaN</td>\n",
       "      <td>135.0</td>\n",
       "      <td>Montreal, PQ / Chesterville, ON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Allison, Mrs. Hudson J C (Bessie Waldo Daniels)</td>\n",
       "      <td>female</td>\n",
       "      <td>25.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>113781</td>\n",
       "      <td>151.5500</td>\n",
       "      <td>C22 C26</td>\n",
       "      <td>S</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Montreal, PQ / Chesterville, ON</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pclass  survived                                             name     sex  \\\n",
       "0       1         1                    Allen, Miss. Elisabeth Walton  female   \n",
       "1       1         1                   Allison, Master. Hudson Trevor    male   \n",
       "2       1         0                     Allison, Miss. Helen Loraine  female   \n",
       "3       1         0             Allison, Mr. Hudson Joshua Creighton    male   \n",
       "4       1         0  Allison, Mrs. Hudson J C (Bessie Waldo Daniels)  female   \n",
       "\n",
       "       age  sibsp  parch  ticket      fare    cabin embarked boat   body  \\\n",
       "0  29.0000      0      0   24160  211.3375       B5        S    2    NaN   \n",
       "1   0.9167      1      2  113781  151.5500  C22 C26        S   11    NaN   \n",
       "2   2.0000      1      2  113781  151.5500  C22 C26        S  NaN    NaN   \n",
       "3  30.0000      1      2  113781  151.5500  C22 C26        S  NaN  135.0   \n",
       "4  25.0000      1      2  113781  151.5500  C22 C26        S  NaN    NaN   \n",
       "\n",
       "                         home.dest  \n",
       "0                     St Louis, MO  \n",
       "1  Montreal, PQ / Chesterville, ON  \n",
       "2  Montreal, PQ / Chesterville, ON  \n",
       "3  Montreal, PQ / Chesterville, ON  \n",
       "4  Montreal, PQ / Chesterville, ON  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def dataframe_maker(config):\n",
    "    file_directory, file_name_titanic = config.values()\n",
    "    os.chdir(file_directory)\n",
    "    df_titanic = pd.read_excel(file_name_titanic)\n",
    "    return df_titanic\n",
    "df_titanic = dataframe_maker(configReader())\n",
    "df = df_titanic\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I explored and inspect this dataset in the privious Assignment, so in this assignment I will just use some of the inspection functions and move forward to the Data Cleaning and Preprocessing. For more information about this dataset one can have a look at 'Assignment6'.\n",
    "\n",
    "This dataset consists of 1309 samples (passengers) and 14 features. Although each feature has the potential to influence the model's outcome, some of them may not significantly impact the model's functionality. Therefore, understanding the nature of each feature becomes crucial for the subsequent steps in the data-science pipeline. This knowledge empowers researchers to discern whether a feature holds importance in the analysis.  Thus, all the features will be described in the following paragraphs.\n",
    "\n",
    "1. **pclass**: presents ticket classes, a potentially significant feature that could have influenced the chances of survival. It is plausible that individuals from higher classes had a better chance of surviving, making this feature relevant for the analysis.\n",
    "\n",
    "2. **survived**: shows the labels that whether a passenger lost his/her life or survived from this disastorous event.\n",
    "\n",
    "3. **name**: contains the names of the passengers.\n",
    "\n",
    "4. **sex**: includes the gender of the passengers, which can significantly influence the model's performance. During emergency conditions, women are often prioritized for protection, making this feature highly relevant and potentially impactful on the model's predictions.\n",
    "\n",
    "5. **age**: This variable represents the age of the passengers, and once again, it can significantly impact the model's outcome. During emergency conditions, children are often given priority for protection, making this feature particularly relevant for predicting survival rates.\n",
    "\n",
    "6. **sibsp**: is the number of siblings and spouse. It can have a slight effect on the outcome of the model. Maybe people with higher number of sibsp had less chance of surviving.\n",
    "\n",
    "7. **parch**: this number preents the number of parents and children. This parameter is quite similar to privious one (sibsp).\n",
    "\n",
    "8. **ticket**: contains the number of passengers' tickets. It seems irrelevant to the topic.\n",
    "\n",
    "9. **fare**: includes the price of the tickets, which can be useful for classifying passengers based on their socio-economic status. Higher ticket prices might indicate a higher social class, while lower prices might correspond to a lower class. This feature can be valuable in categorizing passengers according to their economic standing.\n",
    "\n",
    "10. **cabin**: consists of information about the numbers of cabins for which passengers paid. This feature could be significant as passengers from cabins near the deck might have had an advantage, reaching the surface of the ship more quickly, which could have increased their chances of survival.\n",
    "\n",
    "11. **embarked**: includes the ports in which passengers are get into the ship.\n",
    "\n",
    "12. **boat**: consists of the boats' codes with which some of the passengers survived.\n",
    "\n",
    "13. **body**: contains the body numbers if did not survive and body was recovered. <a href=\"https://github.com/awesomedata/awesome-public-datasets/issues/351\" target=\"_blank\">link</a> \n",
    "\n",
    "14. **home.dest**: contains the home destination of all the the passengers.\n",
    "\n",
    "# Data inspection\n",
    "The most important part of data analyzing pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset has 1309 observations, and 14 variables\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1309 entries, 0 to 1308\n",
      "Data columns (total 14 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   pclass     1309 non-null   int64  \n",
      " 1   survived   1309 non-null   int64  \n",
      " 2   name       1309 non-null   object \n",
      " 3   sex        1309 non-null   object \n",
      " 4   age        1046 non-null   float64\n",
      " 5   sibsp      1309 non-null   int64  \n",
      " 6   parch      1309 non-null   int64  \n",
      " 7   ticket     1309 non-null   object \n",
      " 8   fare       1308 non-null   float64\n",
      " 9   cabin      295 non-null    object \n",
      " 10  embarked   1307 non-null   object \n",
      " 11  boat       486 non-null    object \n",
      " 12  body       121 non-null    float64\n",
      " 13  home.dest  745 non-null    object \n",
      "dtypes: float64(3), int64(4), object(7)\n",
      "memory usage: 143.3+ KB\n",
      "None\n",
      "\n",
      "the total number of null values in this dataset is 3855\n",
      "\n",
      "number of unique groups in pclass is 3\n",
      "groups are:[1, 2, 3]\n",
      "\n",
      "number of unique groups in sex-var is 2\n",
      "groups are:['female', 'male']\n",
      "\n",
      "number of unique groups in sibsp-var is 7\n",
      "groups are:[0, 1, 2, 3, 4, 5, 8]\n",
      "\n",
      "number of unique groups in parch-var is 8\n",
      "groups are:[0, 2, 1, 4, 3, 5, 6, 9]\n",
      "\n",
      "number of unique groups in embarked-var is 4\n",
      "groups are: ['S', 'C', nan, 'Q']\n",
      "\n",
      "The youngest passenger had 0.1667 years old\n",
      "\n",
      "The oldest passenger had 80.0 years old\n",
      "\n",
      "The mean age of the passengers is: 29.881 years old\n",
      "\n",
      "The cheapest ticket costs 0.0\n",
      "\n",
      "The most expensive ticket costs 512.3292\n",
      "\n",
      "The average cost of the tickets is: 33.295\n",
      "\n",
      "the number of passengers who paid the minimum price for the ticket is 17\n"
     ]
    }
   ],
   "source": [
    "def data_inspecter(df):\n",
    "    # find the shape of data\n",
    "    print(f'dataset has {df.shape[0]} observations, and {df.shape[1]} variables\\n')\n",
    "\n",
    "    # finding the information of this dataset\n",
    "    print(f'{df.info()}\\n')\n",
    "\n",
    "    # extract the number of null values of the dataset\n",
    "    null_values = df.isnull().sum().sum()\n",
    "    print(f'the total number of null values in this dataset is {null_values}\\n')\n",
    "\n",
    "    # Find the unique values of different categorical variables\n",
    "    class_values = list(df.pclass.unique())\n",
    "    print(f'number of unique groups in pclass is {len(class_values)}\\ngroups are:{class_values}\\n')\n",
    "\n",
    "    gender_values = list(df.sex.unique())\n",
    "    print(f'number of unique groups in sex-var is {len(gender_values)}\\ngroups are:{gender_values}\\n')\n",
    "\n",
    "    sibsp_values = list(df.sibsp.unique())\n",
    "    print(f'number of unique groups in sibsp-var is {len(sibsp_values)}\\ngroups are:{sibsp_values}\\n')\n",
    "\n",
    "    parch_values = list(df.parch.unique())\n",
    "    print(f'number of unique groups in parch-var is {len(parch_values)}\\ngroups are:{parch_values}\\n')\n",
    "\n",
    "    embark_values = list(df.embarked.unique())\n",
    "    print(f'number of unique groups in embarked-var is {len(embark_values)}\\ngroups are: {embark_values}\\n')\n",
    "\n",
    "    # Find the mean, max and min value of the age colmun\n",
    "    print(f'The youngest passenger had {df.age.min()} years old\\n')\n",
    "    print(f'The oldest passenger had {df.age.max()} years old\\n')\n",
    "    print(f'The mean age of the passengers is: {round(df.age.mean(), 3)} years old\\n')\n",
    "\n",
    "    # Find the mean, max and min value of the tickets\n",
    "    print(f'The cheapest ticket costs {df.fare.min()}\\n')\n",
    "    print(f'The most expensive ticket costs {df.fare.max()}\\n')\n",
    "    print(f'The average cost of the tickets is: {round(df.fare.mean(), 3)}\\n')\n",
    "    print(f'the number of passengers who paid the minimum price for the ticket is {pd.DataFrame(df[df.fare == 0.0]).shape[0]}')\n",
    "\n",
    "data_inspecter(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information about this dataset, one can delve into Assignment6.\n",
    "\n",
    "# Data Cleaning and Preprocessing\n",
    "This section introduces some changes compared to the previous assignment. I will create two separate datasets based on the original data. For the first dataset, I will apply a similar cleaning process as I did in Assignment 6. However, for the second dataset, I will retain certain columns that were excluded from the first dataset due to a significant number of missing values. The reason behind this decision is that certain advanced techniques, such as the Random Forest algorithm, are capable of handling datasets with a substantial amount of missing data. By including these columns, I can take advantage of their potential for better model performance.\n",
    "\n",
    "Moreover, an additional motivation for crafting a dataset similar to the preceding assignment emerges from the establishment of a baseline for comparing outcomes between crafted models using meta-models and a basic Decision Tree. Additionally, the second dataset can serve as a means to compare results obtained through alterations made to the datasets.\n",
    "\n",
    "To sum up, I am creating two distinct datasets: one tailored for methods that struggle with missing data and another designed to leverage the strengths of meta models that can effectively handle such challenges.\n",
    "\n",
    "**First Dataset**\n",
    "\n",
    "Some hanges will be implemented in this dataset.\n",
    "\n",
    "1. Dropping the redundant columns ('name', 'ticket', 'cabin', 'embarked', 'boat', 'body', 'home.dest').\n",
    "\n",
    "2. Summing and merging 'sibsp' and 'parch' columns.\n",
    "\n",
    "3. Filling missing values of 'age' and 'fare' columns.\n",
    "\n",
    "4. Convert 'sex' column label to '0' for 'male' and '1' for 'female'. The reason is that the Decision Tree algorithm cannot convert string to numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pclass</th>\n",
       "      <th>survived</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>family</th>\n",
       "      <th>fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>29.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>211.3375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9167</td>\n",
       "      <td>3</td>\n",
       "      <td>151.5500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>3</td>\n",
       "      <td>151.5500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>3</td>\n",
       "      <td>151.5500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>25.0000</td>\n",
       "      <td>3</td>\n",
       "      <td>151.5500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pclass  survived  sex      age  family      fare\n",
       "0       1         1    1  29.0000       0  211.3375\n",
       "1       1         1    0   0.9167       3  151.5500\n",
       "2       1         0    1   2.0000       3  151.5500\n",
       "3       1         0    0  30.0000       3  151.5500\n",
       "4       1         0    1  25.0000       3  151.5500"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def df_preprocessor_first(df):\n",
    "    corrected_df = df.copy()\n",
    "\n",
    "    # drop the irrelevant columns\n",
    "    corrected_df = corrected_df.iloc[:,:9].drop(columns=['name', 'ticket'])\n",
    "\n",
    "    # merge  'parch' and 'sibsp'\n",
    "    corrected_df.sibsp = corrected_df.loc[:,['sibsp', 'parch']].sum(axis=1)\n",
    "    corrected_df = corrected_df.rename(columns={'sibsp':'family'}).drop(columns=['parch'])\n",
    "\n",
    "    # Fill missing values of 'age' column with mean value\n",
    "    corrected_df.age = corrected_df.age.fillna(value=corrected_df.age.mean())\n",
    "\n",
    "    # Fill the missing value of 'fare' column with mean value\n",
    "    corrected_df.fare = corrected_df.fare.fillna(value=corrected_df.fare.mean())\n",
    "\n",
    "    # Convert 'sex' columns to '0' = male, '1' = 'female'\n",
    "    corrected_df.sex = corrected_df.sex.replace(['male','female'],[0, 1])\n",
    "    return corrected_df\n",
    "\n",
    "first_df = df_preprocessor_first(df)\n",
    "first_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Second Dataset**\n",
    "\n",
    "Certain modifications made in the first dataset will also be incorporated into the second one. Nonetheless, two specific alterations will not be carried over. Primarily, the columns 'boat,' 'cabin,' and 'body' will be retained within the dataset. Secondly, the columns 'parch' and 'sibsp' will not be merged together. While these changes persist, additional adjustments will be applied to the dataset to facilitate its preparation for more in-depth analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pclass</th>\n",
       "      <th>survived</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "      <th>cabin</th>\n",
       "      <th>boat</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>29.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>211.3375</td>\n",
       "      <td>B5</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9167</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>151.5500</td>\n",
       "      <td>C22 C26</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>151.5500</td>\n",
       "      <td>C22 C26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>151.5500</td>\n",
       "      <td>C22 C26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>135.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>25.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>151.5500</td>\n",
       "      <td>C22 C26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pclass  survived  sex      age  sibsp  parch      fare    cabin boat   body\n",
       "0       1         1    1  29.0000      0      0  211.3375       B5    2    NaN\n",
       "1       1         1    0   0.9167      1      2  151.5500  C22 C26   11    NaN\n",
       "2       1         0    1   2.0000      1      2  151.5500  C22 C26  NaN    NaN\n",
       "3       1         0    0  30.0000      1      2  151.5500  C22 C26  NaN  135.0\n",
       "4       1         0    1  25.0000      1      2  151.5500  C22 C26  NaN    NaN"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def df_preprocessor_second(df):    \n",
    "    corrected_df = df.copy()\n",
    "\n",
    "    # drop the irrelevant columns\n",
    "    corrected_df = corrected_df.iloc[:,:-1].drop(columns=['name', 'ticket', 'embarked'])\n",
    "\n",
    "    # Fill missing values of 'age' column with mean value\n",
    "    corrected_df.age = corrected_df.age.fillna(value=corrected_df.age.mean())\n",
    "\n",
    "    # Fill the missing value of 'fare' column with mean value\n",
    "    corrected_df.fare = corrected_df.fare.fillna(value=corrected_df.fare.mean())\n",
    "\n",
    "    # Convert 'sex' columns to '0' = male, '1' = 'female'\n",
    "    corrected_df.sex = corrected_df.sex.replace(['male','female'],[0, 1])\n",
    "    return corrected_df\n",
    "\n",
    "second_df = df_preprocessor_second(df)\n",
    "second_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now one can implement different different meta-models to these dataset and tune their parameters. In the following section, I will first analyze the first dataset, and the move forward to analyze the second dataset.\n",
    "\n",
    "# Implementation of Meta Models on the First Dataset\n",
    "In the previous assignment, the Decision Tree algorithm managed to attain an accuracy of approximately 80% when classifying this dataset. As a result, in this section, I will explore various meta models, employing accuracy as the evaluation criterion, to establish a foundational benchmark for subsequent comparisons.\n",
    "\n",
    "But first, I partition the dataset into separate training and testing sets. It's worth noting that when dealing with a sufficiently large dataset, this division into distinct training and testing subsets is undertaken without encountering the challenge of variance-bias trade-off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training dataset has the following shape: (1112, 5)\n",
      "testing dataset has the following shape: (197, 5)\n"
     ]
    }
   ],
   "source": [
    "# Make X and y\n",
    "X_first = first_df.drop(columns=['survived'])\n",
    "y_first = first_df.survived\n",
    "\n",
    "# Divide the data set into training and testing dataset.\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(X_first, y_first, test_size=0.15)\n",
    "\n",
    "print(f'training dataset has the following shape: {X_train1.shape}')\n",
    "print(f'testing dataset has the following shape: {X_test1.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training dataset contains 1112 datapoints, and the testing dataset contains 197 datapoints.\n",
    "\n",
    "Regarding Implementing meta-models, I will use 'multiple_gridsearch_ensembel' function that can cover tuning of the base estimator and the meta models simultaniously. I will use Bagging, Adaptive Boosting, Gradient Boosting and also Stacking model, and then compare the performance of them by predicting the label of the testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiple_gridsearch_ensembel(estimator_dict,scoring_list, cv_number, refit_method, data_dict):\n",
    "\n",
    "    final_dict = {}\n",
    "\n",
    "    for name in estimator_dict.keys():\n",
    "        \n",
    "        if len(estimator_dict[name][2].keys()) != 0:\n",
    "            estimator_params = estimator_dict[name][2]\n",
    "            best_estimator_dict = {}\n",
    "            counter = 0            \n",
    "\n",
    "            for estimator in estimator_params.keys():\n",
    "\n",
    "                \n",
    "                # make different combination of parameters\n",
    "                param_combinations = product(*[v for k, v in estimator_params[estimator].items()])\n",
    "                params_names = list(estimator_params[estimator].keys())\n",
    "        \n",
    "                for params in param_combinations:\n",
    "                    start = time.time()\n",
    "                    eval_arguments = ', '.join(f'{params_names[i]}={params[i]}' for i in range(len(params_names)))\n",
    "                    eval_statement = (\n",
    "                    f\"{estimator}({eval_arguments})\"\n",
    "                    )\n",
    "                    \n",
    "                    model_estimator = eval(eval_statement)           \n",
    "                    estimator_dict[name][1]['estimator'] = [model_estimator]\n",
    "\n",
    "                    # Create the GridSearchCV object\n",
    "                    grid_search = GridSearchCV(estimator=estimator_dict[name][0], param_grid=estimator_dict[name][1],\n",
    "                                        scoring=scoring_list, cv=cv_number, refit=refit_method)\n",
    "\n",
    "                    grid_search.fit(data_dict['x'], data_dict['y'])\n",
    "\n",
    "                    # Save the best estimator for each model\n",
    "                    best_estimator_dict[counter] = {'best_model': grid_search.best_estimator_,\n",
    "                                                    'best_parameters': grid_search.best_params_,\n",
    "                                                    'best_score': grid_search.best_score_}\n",
    "    \n",
    "                    counter += 1\n",
    "                end = time.time()\n",
    "                print(f'time: {end-start}')\n",
    "\n",
    "            # find the best model in terms of oob_score\n",
    "            best_model_number = max(best_estimator_dict, key=lambda k: best_estimator_dict[k]['best_score'])\n",
    "\n",
    "            # Save the best estimator for each model\n",
    "            final_dict[name] = best_estimator_dict[best_model_number]\n",
    "\n",
    "        else:\n",
    "            # Create the GridSearchCV object\n",
    "            grid_search = GridSearchCV(estimator=estimator_dict[name][0], param_grid=estimator_dict[name][1],\n",
    "                                            scoring=scoring_list, cv=cv_number, refit=refit_method)\n",
    "                \n",
    "            # Fit the the best model to the data\n",
    "            grid_search.fit(data_dict['x'], data_dict['y'])\n",
    "\n",
    "            # Save the best estimator for each model\n",
    "            final_dict[name] = {'best_model': grid_search.best_estimator_,\n",
    "                                'best_parameters': grid_search.best_params_,\n",
    "                                'best_score': grid_search.best_score_}\n",
    "\n",
    "        # order the dictionary based on the magnitude of the scores\n",
    "        final_dict = dict(sorted(final_dict.items(), key=lambda item: -1 * item[1]['best_score']))\n",
    "            \n",
    "    return final_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make parameters dictionaries\n",
    "\n",
    "# Base estimator parameters bagging\n",
    "estimator_params_bagging = {'SVC' : {'C': np.arange(0.01, 1.1, 0.25),\n",
    "                                     'degree': np.arange(2,10,2)},\n",
    "\n",
    "                            'LogisticRegression': {'C':  np.arange(0.01, 10, 0.25)},\n",
    "\n",
    "                            'tree.DecisionTreeClassifier': {'max_depth': [None, 5, 10],\n",
    "                                                            'min_samples_split': [2, 5],\n",
    "                                                            'min_samples_leaf': [1, 2],\n",
    "                                                            'min_impurity_decrease': [0, 1],}}\n",
    "\n",
    "# Base estimator parameters adaptive boosting\n",
    "estimator_params_ada =  {'LogisticRegression': {'C':  np.arange(0.01, 10, 0.25)},\n",
    "                         'tree.DecisionTreeClassifier': {'max_depth': [None, 5, 10],\n",
    "                                                         'min_samples_split': [2, 5],\n",
    "                                                         'min_samples_leaf': [1, 2],\n",
    "                                                         'min_impurity_decrease': [0, 1],}}\n",
    "# BaggingClassifier\n",
    "bagging_param = {'estimator': [SVC(), LogisticRegression(), tree.DecisionTreeClassifier()],\n",
    "                 'n_estimators': np.arange(10,20,2),\n",
    "                 'max_samples': np.arange(350,650,100),\n",
    "                 'max_features': np.arange(5,10,1),\n",
    "                 'bootstrap': [False, True],\n",
    "                 'bootstrap_features': [False, True],\n",
    "                 'n_jobs': [-1]\n",
    "                }\n",
    "\n",
    "\n",
    "# RandomForest Classifier\n",
    "rf_param = {\n",
    "    'n_estimators': np.arange(20, 200, 20),\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [None, 5, 10, 15],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_impurity_decrease': [0, 0.05, 0.1],\n",
    "    'max_samples': [None, 0.05,0.1,0.2],\n",
    "    'n_jobs': [-1]\n",
    "}\n",
    "\n",
    "# AdaptiveBoosting Classifier\n",
    "ada_param = {'estimator': [LogisticRegression(), tree.DecisionTreeClassifier()],\n",
    "                 'n_estimators': np.arange(25, 100, 25)}\n",
    "\n",
    "# GradientBoosting Classifier\n",
    "grad_param = {'learning_rate' : [0.0, 0.5, 1, 10],\n",
    "    'n_estimators': np.arange(20, 200, 20),\n",
    "    'max_depth': [None, 5, 10, 15],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_impurity_decrease': [0, 0.05, 0.1],\n",
    "    'max_features' : ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "# Scoring list\n",
    "scoring_list = ['roc_auc', 'f1', 'accuracy']\n",
    "\n",
    "# Make estimator dictionary\n",
    "estimator_dict={'BaggingClassifier': [BaggingClassifier(), bagging_param, estimator_params_bagging],\n",
    "                'RandomForest': [RandomForestClassifier(), rf_param, {}],\n",
    "                'AdaBoostClassifier': [AdaBoostClassifier(), ada_param, estimator_params_ada],\n",
    "                'GradBoostClassifier': [GradientBoostingClassifier(), grad_param, {}],\n",
    "                }\n",
    "\n",
    "# Make data dictionary\n",
    "data_dict =  {'x': X_train1, \n",
    "              'y': y_train1}\n",
    "\n",
    "best_models_dict1 = multiple_gridsearch_ensembel(estimator_dict, scoring_list, 10, 'accuracy', data_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moreover one can use stacking model for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiple_grid_search(estimator_dict, scoring_list, cv_number, refit_method, data_dict):\n",
    "\n",
    "    final_dict = {}\n",
    "\n",
    "    for name in estimator_dict.keys():\n",
    "        \n",
    "        # Create the GridSearchCV object\n",
    "        grid_search = GridSearchCV(estimator=estimator_dict[name][0], param_grid=estimator_dict[name][1],\n",
    "                                    scoring=scoring_list, cv=cv_number, refit=refit_method)\n",
    "        \n",
    "        # Fit the the best model to the data\n",
    "        grid_search.fit(data_dict['x'], data_dict['y'])\n",
    "\n",
    "        # Save the best estimator for each model\n",
    "        final_dict[name] = {'best_model': grid_search.best_estimator_,\n",
    "                            'best_parameters': grid_search.best_params_,\n",
    "                            'best_score': grid_search.best_score_}\n",
    "\n",
    "    # order the dictionary based on the magnitude of the scores\n",
    "    final_dict = dict(sorted(final_dict.items(), key=lambda item: -1 * item[1]['best_score']))\n",
    "     \n",
    "    return final_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stacking_meta_model(estimator_dict, scoring_list, cv_number, refit_method, data_dict, final_est_list):\n",
    "\n",
    "    start = time.time()\n",
    "    final_dict = multiple_grid_search(estimator_dict, scoring_list, cv_number, refit_method, data_dict)\n",
    "    end = time.time()\n",
    "    print (f'base estimators tunning time is: {end - start}')\n",
    "\n",
    "    estimators_list = [final_dict[model]['best_model'] for model in final_dict.keys()]\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    for estimator in final_est_list:\n",
    "        \n",
    "        # Create the GridSearchCV object\n",
    "        clf = StackingClassifier(estimators=estimators_list, final_estimator=estimator,\n",
    "                                 cv=cv_number, n_jobs=-1)\n",
    "        \n",
    "        # Fit the the best model to the data\n",
    "        clf.fit(data_dict['x'], data_dict['y'])\n",
    "\n",
    "        # Save the best estimator for each model\n",
    "        final_dict[estimator] = {'best_model': clf,\n",
    "                                 'parameters': {'base_estimators': final_dict,\n",
    "                                                'final_estimator': estimator},\n",
    "                                 'score': clf.score(data_dict['x'], data_dict['y'])}\n",
    "    end = time.time()\n",
    "    print (f'model tunning time is: {end - start}')\n",
    "\n",
    "    # order the dictionary based on the magnitude of the scores\n",
    "    model_dict = dict(sorted(final_dict.items(), key=lambda item: -1 * item[1]['best_score']))\n",
    "    best_model = model_dict[list(model_dict.keys())[0]]\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make parameters dictionaries\n",
    "# SVC\n",
    "# one can use all of these parameters, but due to my laptop's power, \n",
    "# I will limit it into a couple of parameters\n",
    "svc_param = {\n",
    "    'SVC__C': [0.1, 0.5, 1, 10],\n",
    "    'SVC__kernel': ['poly', 'rbf'], # 'linear', 'sigmoid'\n",
    "    'SVC__degree': [2, 3, 4],  # Only for 'poly' kernel\n",
    "    #'SVC__gamma': ['scale', 'auto', 0.1, 1, 10],  # 'scale' and 'auto' are for 'rbf' and 'poly' kernels\n",
    "    #'SVC__coef0': [0, 0.1, 0.5],  # Only for 'poly' and 'sigmoid' kernels\n",
    "    #'SVC__tol': [1e-3, 1e-4, 1e-5], \n",
    "}\n",
    "\n",
    "# Create a pipeline with StandardScaler and SVC\n",
    "svc_pipeline = Pipeline([('StandardScaler', StandardScaler()),\n",
    "                                         ('SVC', SVC())])\n",
    "                        \n",
    "# Scoring list\n",
    "scoring_list = ['roc_auc', 'f1', 'accuracy']\n",
    "\n",
    "# Make estimator dictionary\n",
    "estimator_dict={'RandomForest': [RandomForestClassifier(), rf_param],\n",
    "                'SVC': [svc_pipeline, svc_param]}\n",
    "\n",
    "# Make data dictionary\n",
    "data_dict =  {'x': X_train1, \n",
    "              'y': y_train1}\n",
    "# Make a list of final estimators\n",
    "final_est_list = [LogisticRegression(), SVC()]\n",
    "\n",
    "best_model_stacking1 = stacking_meta_model(estimator_dict, scoring_list, 10, 'accuracy', data_dict, final_est_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of Meta Models on the Second Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make X and y\n",
    "X_second = second_df.drop(columns=['survived'])\n",
    "y_second = second_df.survived\n",
    "\n",
    "# Divide the data set into training and testing dataset.\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X_second, y_second, test_size=0.15)\n",
    "\n",
    "print(f'training dataset has the following shape: {X_train2.shape}')\n",
    "print(f'testing dataset has the following shape: {X_test2.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make estimator dictionary\n",
    "estimator_dict={\n",
    "                'RandomForest': [RandomForestClassifier(), rf_param, {}],\n",
    "                'GradBoostClassifier': [GradientBoostingClassifier(), grad_param, {}],\n",
    "                }\n",
    "\n",
    "# Make data dictionary\n",
    "data_dict =  {'x': X_train2, \n",
    "              'y': y_train2}\n",
    "\n",
    "best_model_dict2 = multiple_gridsearch_ensembel(estimator_dict, scoring_list, 10, 'accuracy', data_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
