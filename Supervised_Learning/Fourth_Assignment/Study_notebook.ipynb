{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignemnt 4\n",
    "This notebook serves as a comprehensive guide, explaining crucial mathematical, algorithmic, and logical concepts. For those embarking on a study journey through this series of notebooks, it is recommended to start by reviewing this particular notebook. Familiarizing oneself with the presented concepts here will lay a solid foundation before progressing to the subsequent assignments.\n",
    "\n",
    "## Loss function vs. Cost function\n",
    "In machine learning, 'loss' refers to the difference between the predicted value and the actual value. The 'loss function' is a mathematical function employed during the training phase to quantify this disparity as a single real number. Loss functions are utilized in supervised learning algorithms that employ optimization techniques. Noteworthy examples of such algorithms include regression, logistic regression, and more. It's worth noting that the terms 'cost function' and 'loss function' are often used interchangeably, as they convey similar concepts. However, most of the time there is a slight difference between these two concepts:\n",
    "\n",
    "Loss function: Used when we refer to the error for a single training example.\n",
    "\n",
    "Cost function: Used to refer to an average of the loss functions over an entire training dataset.\n",
    "\n",
    "The cost function plays a crucial role in achieving the optimal solution. It serves as a metric for evaluating the performance of our algorithm or model. By considering both the predicted outputs and the actual outputs, the cost function quantifies the extent to which the model's predictions deviate from the actual values. A higher value indicates a larger discrepancy between the predictions and the actual values. As we fine-tune our model to enhance its predictions, the cost function serves as a gauge of the model's improvement. In essence, this process involves an optimization problem where the objective is to minimize the cost function through various optimization strategies.\n",
    "\n",
    "### Types of the cost function\n",
    "There are many cost functions in machine learning and each has its use cases depending on whether it is a regression problem or classification problem.\n",
    "\n",
    "1. Regression cost Function\n",
    "2. Binary Classification cost Functions\n",
    "3. Multi-class Classification cost Functions\n",
    "\n",
    "In this particular notebook, our focus will be on regression tasks and specifically on a cost function known as Mean Squared Error (MSE). Therefore, in this explanation, I will concentrate solely on this particular case. However, for those interested in delving deeper into the subject, I recommend exploring the material provided in this <a href = 'https://www.analyticsvidhya.com/blog/2021/02/cost-function-is-no-rocket-science/'> link</a> for further study.\n",
    "\n",
    "### Regression cost Function \n",
    "Regression models are employed to predict continuous values, such as an employee's salary, the price of a car, or loan predictions. In the context of regression problems, the corresponding cost function is referred to as the \"Regression Cost Function.\" These cost functions are calculated based on distance-based errors, and are determined as follows:\n",
    "\n",
    "Error = y'-y\n",
    "\n",
    "Where Y is Actual Input, Y' is Predicted output. To continue, MSE (Minimum Squarred Error) will be explained.\n",
    "\n",
    "### MSE (Minimum Squarred Error)\n",
    "this method has the following characteristics:\n",
    "\n",
    "1. To eliminate the possibility of negative errors, the square of the difference between the actual and predicted values is computed. This ensures that all errors are positive and allows for a consistent measure of the discrepancy between the predicted and actual values.\n",
    "\n",
    "2. It is also known as L2 loss.\n",
    "\n",
    "3. In the case of Mean Squared Error (MSE), the squaring of each error serves to penalize even minor deviations in predictions more significantly compared to Mean Absolute Error (MAE). However, it is important to note that if our dataset contains outliers that contribute to larger prediction errors, squaring these errors can amplify them multiple times, resulting in a higher MSE value. This is because the squared error term disproportionately affects larger errors, potentially skewing the overall MSE value.\n",
    "\n",
    "4. Hence we can say that it is less robust to outliers\n",
    "\n",
    "its formalism is as follow\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} ( h_\\theta(x^{(i)}) - y^{(i)} ) ^2 \n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
