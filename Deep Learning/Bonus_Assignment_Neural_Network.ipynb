{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e11ba2a0",
   "metadata": {},
   "source": [
    "# Introduction to Neural Networks\n",
    "\n",
    "In this assignment you first will be introduced to the components of a deep learning model. You will study the code that creates and executes a model and you will apply the theoretical background to propose a model for image data. \n",
    "\n",
    "Learning goals:\n",
    "- understand components of a deep learning model and how they work mathematically\n",
    "- relate the components to the hyperparameters and model setup of keras tensorflow model\n",
    "- propose improvements in design \n",
    "\n",
    "Data:\n",
    "\n",
    "The data we will use is the Breast Cancer Wisconsin (Diagnostic) Data Set of the UCI Machine Learning Repository. You are however free to use your own dataset of interest to study the code. \n",
    "\n",
    "<a name='2'></a>\n",
    "In case you want to study image data classifications you can use a dataset like the Dataset of breast ultrasound imagesfrom Al-Dhabyani W, Gomaa M, Khaled H, Fahmy  2020 Feb;28:104863. DOI: 10.1016/j.dib.2019.104863\n",
    "\n",
    "\n",
    "Sources used: \n",
    "- https://medium.com/mlearning-ai/binary-classification-of-breast-cancer-diagnosis-using-tensorflow-neural-networks-30ac8f40388\n",
    "- Deep Learning for the Life Sciences by Bharath Ramsundar, Peter Eastman, Pat Walters, Vijay Pande Released April 2019 Publisher(s): O'Reilly Media, Inc. ISBN: 9781492039839 https://www.oreilly.com/library/view/deep-learning-for/9781492039822/\n",
    "\n",
    "\n",
    "\n",
    "# Assignment\n",
    "\n",
    "Study the material and answer the questions\n",
    "\n",
    "1. Study the [background text](#0)\n",
    "2. Study the [code steps](#1). Add comments in your own words and explain design choices such as\n",
    "    - number of [layers](#01), \n",
    "    - [width](#02) of layers, \n",
    "    - number of [epochs](#03), \n",
    "    - [activation functions](#04), \n",
    "    - [loss function](#05), \n",
    "    - [gradient descent function](#06), \n",
    "    - [regularization function](#07)\n",
    "3. Run the [code](#1). Evaluate the performance by discussing the results of the evaluation metrics. What hyper parameters would you recommend to change? Explain your choices. \n",
    "4. How do I set up a `batch_size` and how does it effect the outcome? Why do you think the batch_size was not set in the first place?\n",
    "5. (Optional) Would there be a possibility to execute cross validation? How? \n",
    "6. (Optional) How can I introduce a validation test set? What would I need to change in the code?\n",
    "7. Study the [tensor](#2) text. Consider a dataset of breast cancer images. What needs to be changed to the deep learning model design to make a model based on pictures? You can answer this in words, but if you like you can also try to code the solution. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e8e2f0",
   "metadata": {},
   "source": [
    "<a name='0'></a>\n",
    "# Background Deep Learning Models\n",
    "\n",
    "## Linear models\n",
    "\n",
    "One of the simplest model is a linear model\n",
    "\n",
    "$y = \\theta x + b$ in which $x$ is the data, $\\theta$ are the weights and $b$ is the bias vector. Their sizes are determined by the numbers of input and output values. If $x$ has length $m$ and you want $y$ to have lenght $s$ then $\\theta$ will be a $s \\times m$ matrix and $b$ will be a vector of length $s$. By this equation each output is a linear combination of the input components. By setting $\\theta$ and $b$ you can choose any linear combination you want for each component. This model is introduces in 1957 and called perceptron. \n",
    "\n",
    "Unfortunately straight lines often does not fit real datasets. This problem becomes worse in high dimensional data.\n",
    "\n",
    "<a name='01'></a>\n",
    "## Multilayer perceptrons\n",
    "\n",
    "A simple approach to this problem is to stack multiple linear transformations\n",
    "\n",
    "$y = \\theta_2 \\varphi(\\theta_1 x + b_1) + b_2$. Now the result of the ordinary linear transformation $\\theta_1 x + b_1$ is passed through a nonlinear function $\\varphi(x)$. We call this function $\\varphi(x)$ *activation function*. By combining linear with non linear we enable the model to learn a much wider range of functions. \n",
    "\n",
    "And we do not need to stop. We can stack as many as we want on top of each other.\n",
    "\n",
    "$$ h_1 = \\varphi_1(\\theta_1 x + b_1)$$\n",
    "\n",
    "$$ h_2 = \\varphi_2(\\theta_2 h_1 + b_2)$$\n",
    "\n",
    "$$ h_3 = \\varphi_3(\\theta_3 h_2 + b_2)$$\n",
    "\n",
    "$$...$$\n",
    "\n",
    "$$ h_{n-1} = \\varphi_{n-1}(\\theta_{n-1} h_{n-2} + b_{n-1})$$\n",
    "\n",
    "$$ y = \\varphi_n(\\theta_n h_{n-1} + b_n)$$\n",
    "\n",
    "\n",
    "## Neural network\n",
    "\n",
    "Multilayer perceptrons start with an input layer $x$ and information flows from one layer to the next layer resulting in the output layer $y$. This principle is also called *Neural Network*. \n",
    "\n",
    "<a name='04'></a>\n",
    "## Nodes\n",
    "A deep learning node is \"a computational unit that has one or more weighted input connections, a transfer function that combines the inputs in some way, and an output connection. Nodes are then organized into layers to comprise a network\n",
    "\n",
    "<a name='04'></a>\n",
    "## Activation functions \n",
    "Activation functions serve two main purposes: \n",
    "\n",
    "1. to assist a model in accounting for **interaction effects**. An interactive effect occurs when the prediction of one variable, let's say A, is influenced differently based on the value of another variable, let's call it B. To illustrate this, consider a scenario where a model aims to determine whether a certain body weight indicates an increased risk of diabetes. In order to make an accurate prediction, the model needs to take into account the individual's height as well. Some bodyweights may indicate a higher risk for shorter individuals, while signaling good health for taller individuals. Thus, the impact of body weight on diabetes risk is contingent on height, and we can describe weight and height as having an interaction effect.\n",
    "\n",
    "2. Assist a model in capturing **non-linear effects**. This refers to the situation where plotting a variable on the horizontal axis and the corresponding predictions on the vertical axis does not result in a straight line. In other words, the impact of increasing the predictor by one unit varies at different values of that predictor. <a href=\"https://www.kaggle.com/code/dansbecker/rectified-linear-units-relu-in-deep-learning\" target=\"_blank\">link</a>\n",
    "\n",
    "\n",
    "\n",
    "What should the activation function be? Most popular is the *rectified linear unit* (`RelU`) $\\varphi(x) = max(0,x)$ function, which you can use as a default. Another popular function is the `logistic sigmoid` function ($\\varphi(x) = 1/(1 + e^{-x})$) \n",
    "\n",
    "**Rectified Linear Function**: The Rectified Linear Unit is the most commonly used activation function in deep learning models. The function returns 0 if it receives any negative input, but for any positive value x it returns that value back. So it can be written as f(x)=max(0,x). \n",
    "\n",
    "**Sigmoid Functions**: A sigmoid function is a bounded, differentiable, real function that is defined for all real input values and has a non-negative derivative at each point and exactly one inflection point. A sigmoid \"function\" and a sigmoid \"curve\" refer to the same object. <a href=\"https://en.wikipedia.org/wiki/Sigmoid_function\" target=\"_blank\">wiki</a>\n",
    "\n",
    "A sigmoid function is a mathematical function having a characteristic \"S\"-shaped curve or sigmoid curve. A common example of a sigmoid function is the logistic function shown in the first figure and defined by the formula:\n",
    "\n",
    "$$\n",
    "\\varphi(x) = 1/(1 + e^{-x})\n",
    "$$ \n",
    "\n",
    "Other standard sigmoid functions are given in the Examples section. In some fields, most notably in the context of artificial neural networks, the term \"sigmoid function\" is used as an alias for the logistic function. \n",
    "\n",
    "A diverse range of sigmoid functions, such as the logistic and hyperbolic tangent functions, have been employed as activation functions for artificial neurons. Sigmoid curves are also prevalent in statistics, where they serve as cumulative distribution functions that range from 0 to 1. Examples include the integral representations of the logistic density, the normal density, and the Student's t probability density functions. Notably, the logistic sigmoid function is invertible, and its inverse is known as the logit function.\n",
    "\n",
    "Sigmoid functions would seem to have a couple advantages. Even though it gets close to flat, it isn't completely flat anywhere. So it's output always reflects changes in it's input, which we might expect to be a good thing. Secondly, it is non-linear (or curved everywhere). Accounting for non-linearities is one of the activation function's main purposes. So, we expect a non-linear function to work well.\n",
    "\n",
    "However researchers had great difficulty building models with many layers when using the tanh function. It is relatively flat except for a very narrow range (that range being about -2 to 2). The derivative of the function is very small unless the input is in this narrow range, and this flat derivative makes it difficult to improve the weights through gradient descent. This problem gets worse as the model has more layers. This was called the vanishing gradient problem.\n",
    "\n",
    "The ReLU function has a derivative of 0 over half it's range (the negative numbers). For positive inputs, the derivative is 1. When training on a reasonable sized batch, there will usually be some data points giving positive values to any given node. So the average derivative is rarely close to 0, which allows gradient descent to keep progressing.\n",
    "\n",
    "<a name='02'></a>\n",
    "## Deep learning\n",
    "\n",
    "In the model there are two parameters to consider. Width and Depth. The width refers to the size of the layers. We can choose $h_i$ to have any length. They can be larger or smaller then the input and output vectors. Depth refers to the number of layers. When we have only one hidden layer the model is shallow. When we have many layers the model is described as deep, hence deep learning. Often the choice of width and depth is more art then science. \n",
    "<a name='05'></a>\n",
    "## Loss functions\n",
    "\n",
    "To train the model we need a training dataset with a large number of samples $(x,y)$ and an loss function $L(y, \\hat{y})$. To calculate the loss `Euclidean distance` is often used $L(y, \\hat{y}) = \\sqrt{\\sum_i (y_i - \\hat{y}_i)^2}$. When $y$ represents a probability distribution, a popular choise is the `cross entropy` $L(y, \\hat{y}) = -\\sum_i y_i log (\\hat{y}_i)$. We measure the model performance by taking the average loss over every sample. \n",
    "\n",
    "average loss: $\\langle L \\rangle = \\frac{1}{N} \\sum_{i=1}^{N} L(y, \\hat{y})$\n",
    "<a name='06'></a>\n",
    "## Gradient descent\n",
    "Now we have a way to determine how well the model works we need a way to improve it. We search for parameters that minimizes the average loss over the training set. Most work in deep learning use some kind of gradient descent algorithm with learning rate $\\epsilon$. $$\\theta \\mathrel{\\mathop:} = \\theta - \\epsilon \\frac{\\partial}{\\partial\\theta} \\langle L \\rangle$$\n",
    "\n",
    "\n",
    "However with deep learning this takes enormous amount of time. Therefor it is better to use *stochastic gradient descent* (SDG). For every step we take a small set of samples (known as batch) from the training set instead of all samples. The time of each step now is depending on the batch size. The downsite is that is does a lesser job on reducing the loss because it is based on an estimated gradient, not the true gradient. Most deep learning algorithms use SDG. Two of the most popular algorithms are `Adam` and `RMSProp`. \n",
    "\n",
    "**Adam**:\n",
    "\n",
    "The Adam optimizer represents an extended version of stochastic gradient descent that finds utility across a range of deep learning applications, including potential integration into fields like computer vision and natural language processing in the coming years. It made its debut in 2014 and was prominently featured at the ICLR 2015 conference, a gathering of leading deep learning researchers. Operating as an optimization algorithm, Adam offers an intriguing alternative to the conventional stochastic gradient descent process. The name \"Adam\" is a fusion of \"adaptive moment estimation,\" signifying its core principle of leveraging estimations for both the first and second moments of gradients to dynamically adjust the learning rates for individual weights within a neural network. Notably, \"Adam\" is not an acronym but rather a distinct label.\n",
    "\n",
    "Adam has been positioned as an exceptionally efficient stochastic optimization technique, proficiently utilizing only first-order gradients and demanding minimal memory resources. This stands in contrast to its predecessors like AdaGrad and RMSP, which, while showcasing commendable performance gains over SGD in several scenarios, encountered limitations in terms of generalization, sometimes even trailing behind SGD. The introduction of Adam aimed to address these concerns, focusing on enhanced generalization performance.\n",
    "\n",
    "A distinctive characteristic of Adam lies in its hyperparameters, which possess intuitive interpretations, leading to reduced demands for intricate tuning. This design attribute contributes to its accessibility and ease of use, setting it apart as a valuable tool in the deep learning optimization toolkit.\n",
    "\n",
    "**Root Mean Square Propagation (RMSP)**:\n",
    "\n",
    "RMSP is an adaptive optimization algorithm that is an improved version of AdaGrad. RMSP tackles to solve the problems of momentum and works well in online settings. In AdaGrad we take the cumulative summation of squared gradients but, in RMSP we take the 'exponential average'. <a href=\"https://optimization.cbe.cornell.edu/index.php?title=Adam\" target=\"_blank\">link</a> \n",
    "\n",
    "<a name='03'></a>\n",
    "## Epoch and Batch\n",
    "`epoch = 10` means that 10 epochs of gradient descent training will be conducted. \n",
    "During one epoch, the model is presented with each training example once, and the model's parameters are updated based on the loss incurred on those examples. In practical terms, an epoch consists of two main steps:\n",
    "\n",
    "- Forward propagation: Each training example is fed through the model, and the model produces predictions for each example.\n",
    "\n",
    "- Backward propagation (also known as backpropagation): The model calculates the loss between the predicted outputs and the actual labels. The gradients of the loss with respect to the model's parameters are then computed, allowing the model's parameters to be updated in the direction that minimizes the loss.\n",
    "\n",
    "After one epoch is completed, the model has seen and learned from all the training examples once. Typically, multiple epochs are performed to allow the model to further refine its parameters and improve its performance. One can study the following <a href=\"https://www.simplilearn.com/tutorials/machine-learning-tutorial/what-is-epoch-in-machine-learning\" target=\"_blank\">link</a>  for further information.\n",
    "\n",
    "Learning algorithms undergo numerous epochs, ranging from a few tens to potentially exceeding a thousand, in order to meticulously minimize model errors. The scope of epochs can span from as few as ten iterations to as many as a thousand or more. Graphing a learning curve involves juxtaposing data points that correspond to the progression of epochs. This depiction illustrates the relationship between the number of iterations on the x-axis, representing time, and the model's proficiency on the y-axis. By observing this graph, valuable discernments can be gleaned regarding whether the model has encountered issues of underfitting, overfitting, or a balanced fit vis-à-vis the training dataset.\n",
    "\n",
    "The batch size, a pivotal hyperparameter, determines the quantity of samples processed within a given machine learning model prior to updating its internal parameter configuration.\n",
    "\n",
    "Conceptually, a batch corresponds to an iterative loop that iterates over one or more samples, executing predictions for each. These predictions are subsequently juxtaposed against the expected output variables, culminating in the computation of an error. This computed error serves as the basis for refining the model.\n",
    "\n",
    "In practice, a training dataset can be subdivided into multiple batches. The learning approach varies based on batch configuration. When a solitary batch encompasses all the training data, the method is termed batch gradient descent. In the scenario where each batch consists of a single sample, the technique is referred to as stochastic gradient descent. An intermediary approach, termed mini-batch gradient descent, emerges when the batch size ranges from more than one sample to less than the training dataset's overall size.\n",
    "\n",
    "<a name='07'></a>\n",
    "## Regularization\n",
    "\n",
    "To avoid overfitting we use regularization. In deep learning a popular method is called `dropout`. For each layer in the model, you randomly select a subset of elements in the output vector $h_i$ and set them to 0. On every step in the gradient descent, you pick a different random subset. By using dropout you asume that no individual calculation within the model should be too important.\n",
    "\n",
    "Dropout serves as an insightful regularization technique that mimics the concurrent training of multiple neural networks, each designed with unique characteristics. Throughout the training process, a random selection of layer outputs is intentionally disregarded or \"dropped.\" This dynamic adjustment causes the layer to adopt an appearance and functionality distinct from its original design, including a varying count of nodes and connections to the preceding layer. In practice, each update made to a layer during training unfolds with a fresh perspective, effectively injecting an element of diversity into the learning process. This element of dropout introduces controlled noise into training, prompting nodes within a layer to probabilistically assume more or less responsibility for processing inputs.\n",
    "\n",
    "From a conceptual standpoint, dropout's innovative approach can disrupt situations where network layers become overly interdependent, a phenomenon known as co-adaptation, in order to rectify errors propagated from earlier stages. This deliberate disruption contributes to the model's enhanced robustness and adaptability.\n",
    "\n",
    "Dropout is strategically applied on a per-layer basis within a neural network. Its applicability extends across a wide array of layer types, encompassing densely connected, convolutional, and recurrent layers, such as the long short-term memory (LSTM) network layer. It is noteworthy that dropout can be employed on any or all of the network's hidden layers, as well as the visible or input layer, while being deliberately excluded from the output layer to maintain consistency in predictions.\n",
    "\n",
    "## Hyperparameters optimization\n",
    "\n",
    "In summary there are a lot of choices to make. Such as\n",
    "\n",
    "- Number of layers in the model\n",
    "- Width of each layer\n",
    "- Activation function\n",
    "- Learning rate\n",
    "- Batch size\n",
    "- Loss function\n",
    "- Number of epochs\n",
    "- Number of elements to set to 0 when using dropout\n",
    "\n",
    "\n",
    "## Using validation set\n",
    "Ideally we want a low loss on the test set. But we cannot use the test set for training. We can use another approach however, with a validation set. \n",
    "\n",
    "- For each set of hyperparameters train the model on the training set and compute loss on validation set. \n",
    "- Whichever set of hyperparameters give the lowest loss on the validation set, accept them as your final model\n",
    "- Evaluate that final model on the test set to get an unbiased measure\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6752b5c",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "# Background Tensors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d9ac3361",
   "metadata": {},
   "source": [
    "Tensors are a fundamental concept in the field of mathematics and play a vital role in understanding and manipulating data in various dimensions. \n",
    "\n",
    "### scalar\n",
    "A scalar is a **0D** tensor, representing a single number. It has no dimensions or axes. Scalars are the simplest form of data representation\n",
    "\n",
    "### vector\n",
    "Moving on to the next level, we encounter vectors. Vectors are **1D** tensors consisting of an array of values along a single axis. \n",
    "\n",
    "### matrix\n",
    "The next type of tensor is the matrix, which is a **2D** tensor. Matrices are arranged in rows and columns and can be thought of as a rectangular grid of numbers. They are extensively used in various mathematical operations, such as linear transformations and matrix multiplications. In machine learning the feature matrix X is a 2D tensors of shape(samples, features). \n",
    "\n",
    "\n",
    "### 3D tensor\n",
    "When we pack multiple matrices together in a new array, we obtain a 3D tensor. This tensor can be visually interpreted as a cube of numbers, with three axes representing depth, height, and width. Timeseries data is often represented by a 3D tensors with shape (samples, timesteps, features) \n",
    "\n",
    "\n",
    "### 4D and beyond\n",
    "By further extending this concept, we can create even higher-dimensional tensors. For instance, packing 3D tensors into an array gives rise to a 4D tensor. Similarly, the process can be repeated to form 5D tensors and beyond\n",
    "\n",
    "A single image has 3 dimensions, height, width and color-channel. A dataset with multiple images is a 4D tensor of shape (samples, height, width, channels) or (samples, channels, height, width). \n",
    "\n",
    "In the case of videos we even have a 5D tensors of shape (samples, frames, height, width, channels) or (samples, frames, channels, height, width)\n",
    "\n",
    "\n",
    "### Tensor transformation\n",
    "All transformations learned by deep neural networks can be reduced to a handful of tensor operations applied to tensors of numeric data. For instance, it’s possible to add tensors, multiply tensors, and so on.\n",
    "A tensor operation example is a relu activation function. A layer is a data-processing module that takes as input one or more tensors and that outputs one or more tensors. The layer’s weights learned with gradient descent.\n",
    "\n",
    "Learning happens by drawing random batches of data samples and their targets, and computing the gradient of the network parameters with respect to the loss on the batch. The network parameters are then moved a bit (the magnitude of the move is defined by the learning rate) in the opposite direction from the gradient\n",
    "\n",
    "The entire learning process is made possible by the fact that neural networks are chains of differentiable tensor operations, and thus it’s possible to apply the chain rule of derivation to find the gradient function mapping the current parameters and current batch of data to a gradient value.\n",
    "\n",
    "\n",
    "### Layers\n",
    "Different layers are appropriate for different tensor formats and different types of data processing. For instance, simple vector data, stored in 2D tensors of shape (samples, features), is often processed by densely connected layers, also called fully connected or **dense layers** (the `Dense` class in Keras)\n",
    "Sequence data, stored in 3D tensors of shape (samples, timesteps, features), is typically processed by **recurrent layers** such as an `LSTM` layer. \n",
    "Image data, stored in 4D tensors, is usually processed by 2D **convolution layers** `(Conv2D)`.\n",
    "\n",
    "Continuing with this notebook, my initial focus involves elucidating the core concepts of Fully Connected Neural Networks (FCNNs) and Convolutional Neural Networks (CNNs), while also highlighting their distinctions. Following this, I will proceed to outline a comprehensive guide on effectively harnessing the capabilities of the Keras package.\n",
    "\n",
    "### FCNN vs. CNN\n",
    "Understanding the architecture of a neural network can be challenging initially. Crafting a neural network entails making a multitude of design decisions, including determining input and output dimensions for each layer, strategically placing batch normalization and dropout layers, selecting appropriate activation functions, and more. In this discussion, I aim to delve into the underlying mechanics of fully connected layers and convolutional operations, while also exploring the methodology to compute output dimensions for convolutional layers. <a href=\"https://builtin.com/machine-learning/fully-connected-layer\" target=\"_blank\">link</a> \n",
    "\n",
    "In recent years, the field of deep learning has experienced a remarkable surge, propelled by enhanced computational capabilities and advancements in model architecture. When delving into the realm of deep learning, two prominent network types frequently discussed are fully connected neural networks (FCNNs) and convolutional neural networks (CNNs). Serving as foundational pillars for deep learning frameworks, these networks lay the groundwork upon which numerous other intricate neural architectures are built. To facilitate comprehension, this discussion will commence by elucidating the mechanics of fully connected layers, subsequently demystify convolutional layers.\n",
    "\n",
    "**FCNN**:\n",
    "\n",
    "A fully connected layer refers to a neural network in which each neuron applies a linear transformation to the input vector through a weights matrix. As a result, all possible connections layer-to-layer are present, meaning every input of the input vector influences every output of the output vector. However, not all weights affect all outputs. The weights of a neuron only affect one output such as output A, and do not have an effect on other outputs like outputs B, C or D.\n",
    "\n",
    "**CNN**:\n",
    "\n",
    "A convolution can be likened to a sliding dot product, where a kernel traverses the input matrix, computing dot products as if they were vectors. Unlike dense connections, convolutions exhibit a sparse connectivity pattern, where not every input node influences all output nodes. This attribute grants convolutional layers heightened adaptability in the learning process. Furthermore, the reduced quantity of weights per layer proves advantageous, particularly when dealing with high-dimensional inputs like image data. These merits collectively underlie the renowned capability of Convolutional Neural Networks (CNNs) to discern and absorb intricate features from data, including shapes and textures within images.\n",
    "\n",
    "**How to Work With Fully Connected Layers and Convolutional Neural Networks**\n",
    "\n",
    "In the case of Fully Connected (FC) layers, determining the output size is straightforward—simply by selecting the number of columns in the weights matrix. However, this simplicity doesn't extend to convolutional layers. Convolutions encompass a multitude of parameters that can be adjusted to tailor the resulting output size of the operation.\n",
    "\n",
    "To determine the output size of the convolution, the following equation can be applied:\n",
    "\n",
    "$$\n",
    "n_{out}=\\frac{n_{in}-2p-k}{s} + 1\n",
    "$$\n",
    "\n",
    "Where, $n_{out}$ is the number of output layer, $n_{in}$ is the number of input layers, p is the **padding** number, k is the **kernel** number, and s is **stride** number. I explained the concept of kernel fully in the privious assignments, so here I just describe **padding** and **stride**.\n",
    "\n",
    "**Padding**: Padding is a technique employed that guarantees that the input data has the exact size and shape that the model anticipates after every convolutional operation at each stage of the deep learning model. In order to work the kernel with processing in the image, padding is added to the outer frame of the image to allow for more space for the filter to cover in the image. Adding padding to an image processed by a CNN allows for a more accurate analysis of images. <a href=\"https://www.baeldung.com/cs/deep-neural-networks-padding\" target=\"_blank\">link1</a>, <a href=\"https://analyticsindiamag.com/guide-to-different-padding-methods-for-cnn-models/\" target=\"_blank\">link2</a>\n",
    "\n",
    "**Stride**: Stride is a parameter of the neural network's filter that modifies the amount of movement over the image or video. For example, if a neural network's stride is set to 1, the filter will move one pixel, or unit, at a time. <a href=\"https://medium.com/machine-learning-algorithms/what-is-stride-in-convolutional-neural-network-e3b4ae9baedb\" target=\"_blank\">link</a>\n",
    "\n",
    "The resultant size of the output is determined by the input size, with the addition of twice the padding, minus the kernel size, divided by the stride, plus one. This equation is frequently applied to square matrices, yielding the same value for both rows and columns. In cases where the division does not yield an integer, the value is rounded up to the nearest whole number. \n",
    "\n",
    "The division by the stride is logical, as it reflects the reduction in output size due to the skipped operations. The inclusion of twice the padding arises from the padding being applied on both ends of the matrix, effectively accounting for the padding twice.\n",
    "\n",
    "Based on the provided equation, the resulting output size tends to be either equal to or smaller than the input size, unless significant padding is applied. However, excessive padding can lead to challenges in learning, as it creates sparse inputs for each layer. To address this issue, transposed convolutions are employed to effectively expand the input dimensionality and overcome the sparse input problem. The mentioned technique utilizes the following formula to calculate the number of output:\n",
    "\n",
    "$$\n",
    "n_{out}=(n_{in}-1)-2p+(k-1) + 1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1645f5b",
   "metadata": {},
   "source": [
    "# Keras\n",
    "\n",
    "Keras serves as a neural network Application Programming Interface (API) designed for Python, closely integrated with TensorFlow, a framework utilized for constructing machine learning models. Keras' models provide an intuitive and user-friendly approach to outlining a neural network structure. TensorFlow subsequently takes charge of constructing the network according to the specified architecture through the Keras API. <a href=\"https://www.activestate.com/resources/quick-reads/what-is-a-keras-model/\" target=\"_blank\">link</a>\n",
    "\n",
    "TensorFlow stands as an open-source collection of libraries designed to facilitate the creation and manipulation of neural networks, particularly those applied in Machine Learning (ML) and Deep Learning endeavors.\n",
    "\n",
    "In contrast, Keras functions as a high-level Application Programming Interface (API) built atop TensorFlow. Keras streamlines the development of intricate neural network structures by providing an accessible and user-friendly framework. It abstracts and simplifies the process of implementing complex neural networks, allowing users to design models with greater ease and efficiency.\n",
    "\n",
    "### When to Use Keras vs TensorFlow \n",
    "TensorFlow serves as a comprehensive machine learning platform, encompassing a spectrum of capabilities spanning from high-level to low-level functionalities, catering to the construction and deployment of machine learning models. Nonetheless, it does present a notable learning curve. It is particularly well-suited for scenarios where the following requirements are present:\n",
    "\n",
    "1. Deep Learning Research: TensorFlow is highly conducive to in-depth exploration and experimentation in the realm of deep learning.\n",
    "\n",
    "2. Complex Neural Networks: Its capabilities shine when dealing with intricate and multifaceted neural network architectures.\n",
    "\n",
    "3. Large Datasets: TensorFlow is aptly equipped for managing and processing substantial datasets, making it suitable for tasks involving extensive data.\n",
    "\n",
    "4. High-Performance Models: It excels in the creation of models geared towards achieving high levels of performance and computational efficiency.\n",
    "\n",
    "Conversely, Keras is an ideal choice for individuals who lack an extensive foundation in Deep Learning but still aspire to engage with neural networks. With Keras, you can swiftly and effortlessly construct a neural network model using minimal code, facilitating the process of rapid prototyping. It offers a user-friendly interface that simplifies the creation of neural network architectures without requiring an in-depth understanding of the intricacies of Deep Learning. Moreover, Keras is often considered to be less error-prone than TensorFlow, leading to increased likelihood of model accuracy when using Keras as compared to TensorFlow. This is because Keras provides a higher level of abstraction and encapsulation of complex operations, which can reduce the potential for implementation errors and streamline the model development process. As a result, users may find it easier to achieve accurate models with Keras due to its emphasis on simplicity and user-friendly design.\n",
    "\n",
    "### Keras Models\n",
    "When working with Keras, the central entities are the models. These models serve as the foundation for defining TensorFlow neural networks, encompassing attributes, functions, and layers according to your requirements.\n",
    "\n",
    "Keras provides several distinct APIs that enable the specification of your neural network structure, comprising:\n",
    "\n",
    "1. **Sequential API**: This approach involves creating a model layer by layer, making it well-suited for many problem types. It's simple and straightforward, resembling a list of layers. However, it is limited to constructing single-input, single-output layer stacks.\n",
    "\n",
    "2. **Functional API**: Offering a comprehensive and versatile interface, the Functional API supports a wide range of model architectures. It allows for greater complexity and flexibility compared to the sequential approach.\n",
    "\n",
    "3. **Model Subclassing**: This advanced technique empowers you to build models entirely from the ground up. While suitable for intricate research and specialized scenarios, it is less commonly used in practical applications.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0ab640",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "# Study Case\n",
    "\n",
    "Consider the Breast Cancer Wisconsin (Diagnostic) Data Set. UCI Machine Learning Repository: Breast Cancer Wisconsin (diagnostic) data set. (n.d.). https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29. Consider the code below. \n",
    "\n",
    "I have studied this dataset in Assignment5. Consequenly, I will use its inspection part here.\n",
    "\n",
    "### Data Loading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e3bcdd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General modules\n",
    "import yaml\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data modules\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Preprocessing modules\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Deep learning modules\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Dense, Conv2D\n",
    "\n",
    "# Evaluation modules\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "584aed71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inspired by https://fennaf.gitbook.io/bfvm22prog1/data-processing/configuration-files/yaml\n",
    "\n",
    "def configReader():\n",
    "    \"\"\"\n",
    "    explanation: This function open config,yaml file \n",
    "    and fetch the gonfigue file information\n",
    "    input: ...\n",
    "    output: configue file\n",
    "    \"\"\"\n",
    "    with open(\"config.yaml\", \"r\") as inputFile:\n",
    "        config = yaml.safe_load(inputFile)\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c762909",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0    842302         M        17.99         10.38          122.80     1001.0   \n",
       "1    842517         M        20.57         17.77          132.90     1326.0   \n",
       "2  84300903         M        19.69         21.25          130.00     1203.0   \n",
       "3  84348301         M        11.42         20.38           77.58      386.1   \n",
       "4  84358402         M        20.29         14.34          135.10     1297.0   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0          0.11840           0.27760          0.3001              0.14710   \n",
       "1          0.08474           0.07864          0.0869              0.07017   \n",
       "2          0.10960           0.15990          0.1974              0.12790   \n",
       "3          0.14250           0.28390          0.2414              0.10520   \n",
       "4          0.10030           0.13280          0.1980              0.10430   \n",
       "\n",
       "   ...  radius_worst  texture_worst  perimeter_worst  area_worst  \\\n",
       "0  ...         25.38          17.33           184.60      2019.0   \n",
       "1  ...         24.99          23.41           158.80      1956.0   \n",
       "2  ...         23.57          25.53           152.50      1709.0   \n",
       "3  ...         14.91          26.50            98.87       567.7   \n",
       "4  ...         22.54          16.67           152.20      1575.0   \n",
       "\n",
       "   smoothness_worst  compactness_worst  concavity_worst  concave points_worst  \\\n",
       "0            0.1622             0.6656           0.7119                0.2654   \n",
       "1            0.1238             0.1866           0.2416                0.1860   \n",
       "2            0.1444             0.4245           0.4504                0.2430   \n",
       "3            0.2098             0.8663           0.6869                0.2575   \n",
       "4            0.1374             0.2050           0.4000                0.1625   \n",
       "\n",
       "   symmetry_worst  fractal_dimension_worst  \n",
       "0          0.4601                  0.11890  \n",
       "1          0.2750                  0.08902  \n",
       "2          0.3613                  0.08758  \n",
       "3          0.6638                  0.17300  \n",
       "4          0.2364                  0.07678  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def dataframe_maker(config):\n",
    "    file_directory, file_name_titanic = config.values()\n",
    "    os.chdir(file_directory)\n",
    "    df_titanic = pd.read_csv(file_name_titanic).drop('Unnamed: 32', axis=1)\n",
    "    return df_titanic\n",
    "\n",
    "df = dataframe_maker(configReader())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3219b9b",
   "metadata": {},
   "source": [
    "This dataset consists of 569 samples with 32 features. I have inspected and analyzed the dataset in Assignment 5. Therefore, for this assignment, you can refer to the aforementioned Assignment for a comprehensive overview. In this context, I will employ the inspector function to showcase a selection of its distinctive attributes.\n",
    "\n",
    "### Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa052148",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspecting_data(df):\n",
    "\n",
    "    # find the shape of data\n",
    "    print(f'dataset has {df.shape[0]} observations, and {df.shape[1]} variables\\n')\n",
    "\n",
    "    # finding the information of this dataset\n",
    "    print(f'{df.info()}\\n')\n",
    "\n",
    "    # extract the number of null values of the dataset\n",
    "    null_values = df.isnull().sum().sum()\n",
    "    print(f'the total number of null values in this dataset is {null_values}\\n')\n",
    "\n",
    "    # find whether the number of unique ids is equel to the number of observations\n",
    "    if df.id.unique().shape[0] == df.shape[0]:\n",
    "        print(f'the number of unique IDs is {df.shape[0]}, which it is equal to the number of observations\\n')\n",
    "    \n",
    "    # find the distributaion of the datapoints in the label column\n",
    "    member_numbers = df.diagnosis.value_counts()\n",
    "    print(f'number of members in each diagnosis category')\n",
    "    print(f'{member_numbers}\\n')\n",
    "    print(f'Benign (B): {(member_numbers[0] / df.shape[0]).round(3)}%')\n",
    "    print(f'Malignant (M): {(member_numbers[1] / df.shape[0]).round(3)}%\\n')\n",
    "\n",
    "    # find the number of negative or zero datapoints \n",
    "    print(f'This dataset contains {df.iloc[:,2:][df.iloc[:,2:]<0].count().sum()} negative values\\n')\n",
    "    print(f'This dataset contains {df.iloc[:,2:][df.iloc[:,2:]==0].count().sum()} zero values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f218f271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset has 569 observations, and 32 variables\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 569 entries, 0 to 568\n",
      "Data columns (total 32 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   id                       569 non-null    int64  \n",
      " 1   diagnosis                569 non-null    object \n",
      " 2   radius_mean              569 non-null    float64\n",
      " 3   texture_mean             569 non-null    float64\n",
      " 4   perimeter_mean           569 non-null    float64\n",
      " 5   area_mean                569 non-null    float64\n",
      " 6   smoothness_mean          569 non-null    float64\n",
      " 7   compactness_mean         569 non-null    float64\n",
      " 8   concavity_mean           569 non-null    float64\n",
      " 9   concave points_mean      569 non-null    float64\n",
      " 10  symmetry_mean            569 non-null    float64\n",
      " 11  fractal_dimension_mean   569 non-null    float64\n",
      " 12  radius_se                569 non-null    float64\n",
      " 13  texture_se               569 non-null    float64\n",
      " 14  perimeter_se             569 non-null    float64\n",
      " 15  area_se                  569 non-null    float64\n",
      " 16  smoothness_se            569 non-null    float64\n",
      " 17  compactness_se           569 non-null    float64\n",
      " 18  concavity_se             569 non-null    float64\n",
      " 19  concave points_se        569 non-null    float64\n",
      " 20  symmetry_se              569 non-null    float64\n",
      " 21  fractal_dimension_se     569 non-null    float64\n",
      " 22  radius_worst             569 non-null    float64\n",
      " 23  texture_worst            569 non-null    float64\n",
      " 24  perimeter_worst          569 non-null    float64\n",
      " 25  area_worst               569 non-null    float64\n",
      " 26  smoothness_worst         569 non-null    float64\n",
      " 27  compactness_worst        569 non-null    float64\n",
      " 28  concavity_worst          569 non-null    float64\n",
      " 29  concave points_worst     569 non-null    float64\n",
      " 30  symmetry_worst           569 non-null    float64\n",
      " 31  fractal_dimension_worst  569 non-null    float64\n",
      "dtypes: float64(30), int64(1), object(1)\n",
      "memory usage: 142.4+ KB\n",
      "None\n",
      "\n",
      "the total number of null values in this dataset is 0\n",
      "\n",
      "the number of unique IDs is 569, which it is equal to the number of observations\n",
      "\n",
      "number of members in each diagnosis category\n",
      "diagnosis\n",
      "B    357\n",
      "M    212\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Benign (B): 0.627%\n",
      "Malignant (M): 0.373%\n",
      "\n",
      "This dataset contains 0 negative values\n",
      "\n",
      "This dataset contains 78 zero values\n"
     ]
    }
   ],
   "source": [
    "inspecting_data(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adaccadc",
   "metadata": {},
   "source": [
    "During the data inspection phase, it was observed that the dataset consists of 30 float features, one integer column representing the ID, and a categorical column containing the labels. \n",
    "\n",
    "Before entering the next step, I found it valuable to compare the mean values of features for these two categories, and find whether there is any different between their mean values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17aa7c3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>diagnosis</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>B</th>\n",
       "      <td>2.654382e+07</td>\n",
       "      <td>12.146524</td>\n",
       "      <td>17.914762</td>\n",
       "      <td>78.075406</td>\n",
       "      <td>462.790196</td>\n",
       "      <td>0.092478</td>\n",
       "      <td>0.080085</td>\n",
       "      <td>0.046058</td>\n",
       "      <td>0.025717</td>\n",
       "      <td>0.174186</td>\n",
       "      <td>...</td>\n",
       "      <td>13.379801</td>\n",
       "      <td>23.515070</td>\n",
       "      <td>87.005938</td>\n",
       "      <td>558.899440</td>\n",
       "      <td>0.124959</td>\n",
       "      <td>0.182673</td>\n",
       "      <td>0.166238</td>\n",
       "      <td>0.074444</td>\n",
       "      <td>0.270246</td>\n",
       "      <td>0.079442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>M</th>\n",
       "      <td>3.681805e+07</td>\n",
       "      <td>17.462830</td>\n",
       "      <td>21.604906</td>\n",
       "      <td>115.365377</td>\n",
       "      <td>978.376415</td>\n",
       "      <td>0.102898</td>\n",
       "      <td>0.145188</td>\n",
       "      <td>0.160775</td>\n",
       "      <td>0.087990</td>\n",
       "      <td>0.192909</td>\n",
       "      <td>...</td>\n",
       "      <td>21.134811</td>\n",
       "      <td>29.318208</td>\n",
       "      <td>141.370330</td>\n",
       "      <td>1422.286321</td>\n",
       "      <td>0.144845</td>\n",
       "      <td>0.374824</td>\n",
       "      <td>0.450606</td>\n",
       "      <td>0.182237</td>\n",
       "      <td>0.323468</td>\n",
       "      <td>0.091530</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id  radius_mean  texture_mean  perimeter_mean  \\\n",
       "diagnosis                                                            \n",
       "B          2.654382e+07    12.146524     17.914762       78.075406   \n",
       "M          3.681805e+07    17.462830     21.604906      115.365377   \n",
       "\n",
       "            area_mean  smoothness_mean  compactness_mean  concavity_mean  \\\n",
       "diagnosis                                                                  \n",
       "B          462.790196         0.092478          0.080085        0.046058   \n",
       "M          978.376415         0.102898          0.145188        0.160775   \n",
       "\n",
       "           concave points_mean  symmetry_mean  ...  radius_worst  \\\n",
       "diagnosis                                      ...                 \n",
       "B                     0.025717       0.174186  ...     13.379801   \n",
       "M                     0.087990       0.192909  ...     21.134811   \n",
       "\n",
       "           texture_worst  perimeter_worst   area_worst  smoothness_worst  \\\n",
       "diagnosis                                                                  \n",
       "B              23.515070        87.005938   558.899440          0.124959   \n",
       "M              29.318208       141.370330  1422.286321          0.144845   \n",
       "\n",
       "           compactness_worst  concavity_worst  concave points_worst  \\\n",
       "diagnosis                                                             \n",
       "B                   0.182673         0.166238              0.074444   \n",
       "M                   0.374824         0.450606              0.182237   \n",
       "\n",
       "           symmetry_worst  fractal_dimension_worst  \n",
       "diagnosis                                           \n",
       "B                0.270246                 0.079442  \n",
       "M                0.323468                 0.091530  \n",
       "\n",
       "[2 rows x 31 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this table shows that these two cance categories (M, B) are completely different from each other\n",
    "df.copy().groupby('diagnosis').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195962f5",
   "metadata": {},
   "source": [
    "Almost all the malignant tumours' features are greater than Benign tumours, especially when it comes to spatial attributes. It shows that Malignant tumours are in average larger than their benign counterparts. Also, these tumours are more concave and compact. However, there is no clear evidece of any difference in the smoothness of the tumours from these two categories. Based on the table one can see the difeerence between values of different features is evident. To continue, the next step will be preprocessing.\n",
    "\n",
    "### Preprocessing Data\n",
    "In this section, I've opted not to perform preprocessing independently. Instead, I've employed the same operations that our instructor utilized in the educational notebook.\n",
    "\n",
    "Therefore, the initial step involves utilizing the <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html\" target=\"_blank\">LabelEncoder</a> library to convert the 'diagnosis' column (label column) into binary values '0' and '1'. Subsequently, the dataset will be partitioned into training and testing subsets, with the testing set accounting for 20% of the original dataset. Lastly, both the training and testing datasets will undergo normalization using the <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html\" target=\"_blank\">MinMaxScaler</a> library, ensuring that the entire feature range is transformed to the interval [0, 1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "331b8e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the label column\n",
    "le = LabelEncoder()\n",
    "le.fit(df['diagnosis'])\n",
    "df['diagnosis'] = le.transform(df['diagnosis'])\n",
    "\n",
    "# Seperate the features and label from the dataset\n",
    "X = df[df.columns[2:]]\n",
    "y = df['diagnosis']\n",
    "\n",
    "# Partitioned the feature matrix into training and testing dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "  X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "# Normalize the datasets using MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae82469",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e3d74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(20, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "\n",
    "model.fit(x=X_train, y=y_train, epochs=100, validation_data=(X_test, y_test))\n",
    "\n",
    "model_loss = pd.DataFrame(model.history.history)\n",
    "model_loss.plot()\n",
    "\n",
    "predicted=(model.predict(X_test) > 0.5).astype(int)\n",
    "print(classification_report(y_test, predicted))\n",
    "\n",
    "confusion_matrix = confusion_matrix(y_test, predicted)\n",
    "cm_display = ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, \n",
    "                                    display_labels = [False, True])\n",
    "\n",
    "cm_display.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ab35c5",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378a49b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
